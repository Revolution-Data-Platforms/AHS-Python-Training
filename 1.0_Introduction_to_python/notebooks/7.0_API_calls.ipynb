{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest APIs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST (Representational State Transfer) API is a web-based service that uses HTTP methods to get, send, delete, or update data. Many online platforms provide REST APIs to interact with their data, and in most cases, this data is returned in JSON format.\n",
    "\n",
    "To work with REST APIs, you will need a package for sending HTTP requests. Python's requests package is a great choice for this task. You can install it using pip:\n",
    "\n",
    "`pip install requests`\n",
    "\n",
    "Let's assume we have a REST API that returns a JSON response. In our example, we will use a public API that provides information about users: https://jsonplaceholder.typicode.com/users.\n",
    "\n",
    "Here is how you can read this data into a pandas DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Define the API endpoint\n",
    "url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# Send a GET request to the REST API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check that the GET request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response to a Python dictionary\n",
    "    data = response.json()\n",
    "\n",
    "    # Convert the dictionary to a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will output a DataFrame where each row corresponds to a user, and columns correspond to user attributes (id, name, username, etc.). If you want to read only a specific attribute from the API response, you can do it by selecting this attribute when creating the DataFrame. Let's say you are interested in the names and email addresses of the users only:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"name\", \"email\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that not all APIs are public and some require authentication. The procedure to authenticate will depend on the specific API, so always refer to the API's documentation for instructions. Also, always be aware of the API's rate limits. If you send too many requests in a short period of time, the server might block your IP address. To avoid this, you can add pauses between your requests using the `time.sleep()` function. Finally, keep in mind that API responses can be large, and downloading and processing them can take a lot of time. Consider filtering the data on the server side (if the API supports this) or processing the data in chunks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is the process of extracting data from websites. Python, with its rich ecosystem of libraries, is a popular choice for web scraping tasks. In this tutorial, we'll cover the basics of web scraping using Python and two popular libraries: Requests and Beautiful Soup.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the Libraries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you need to install Requests and Beautiful Soup libraries. To do this, run the following command:\n",
    "\n",
    "```bash\n",
    "pip install requests beautifulsoup4\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching a Web Page\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to fetch the web page's content. We'll use the Requests library for this purpose. In this tutorial, we'll use the [Wikipedia page for \"List of state and union territory capitals in India\"](https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India) as an example. The following code fetches the content of the web page and stores it in the `page_content` variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests) (2.2.2)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Installing collected packages: certifi, requests\n",
      "Successfully installed certifi-2024.8.30 requests-2.32.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_state_and_union_territory_capitals_in_India\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Page successfully fetched!\", end=\"\\n\\n\")\n",
    "    page_content = response.text\n",
    "    print(f\"Object type: {type(page_content)}\", end=\"\\n\\n\")\n",
    "    print(\"Page content:\", end=\"\\n\\n\")\n",
    "    print(page_content)\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: unable to fetch the page.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Web Page\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the HTML content, we need to parse it to navigate and extract the data. Beautiful Soup will help us with this task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "\n",
    "print(f\"Object type: {type(soup)}\", end=\"\\n\\n\")\n",
    "print(\"Page content:\", end=\"\\n\\n\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After parsing the HTML, we can now locate and extract the desired information using Beautiful Soup. Let's say we want to extract all the headings (h1, h2, and h3 tags) from the page. We can do this using the `find_all` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = soup.find_all([\"h1\", \"h2\", \"h3\"])\n",
    "\n",
    "for heading in headings:\n",
    "    print(heading.get_text())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a specific heading by using the `find` method and passing the tag id as a keyword argument:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_heading = soup.find(id=\"List\")\n",
    "\n",
    "print(specific_heading.get_text())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Tables to Pandas DataFrames\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contains tables with of states and union territories. We can extract the tables using the `find_all` method of Beautiful Soup and convert them to a list of Pandas DataFrames using the `read_html` method of Pandas.\n",
    "\n",
    "Note you may need to install the `lxml` and `html5lib` libraries to use the `read_html` method. You can install them using the following command:\n",
    "\n",
    "```bash\n",
    "pip install lxml html5lib\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "df_list = pd.read_html(str(tables))\n",
    "\n",
    "for df in df_list:\n",
    "    display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the following resources for more information on importing data into Python:\n",
    "\n",
    "- [pandas: How to Read and Write Files](https://realpython.com/pandas-read-write-fi)\n",
    "- [pandas Documentation: IO Tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)\n",
    "- [REST APIs with Python](https://realpython.com/api-integration-in-python/)\n",
    "- [Web Scraping with Python: A Beginner's Guide](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "- [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
    "- [Python Web Scraping Tutorial](https://www.freecodecamp.org/news/how-to-scrape-websites-with-python-2/)\n",
    "- [Requests Documentation](https://requests.readthedocs.io/en/master/)\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
