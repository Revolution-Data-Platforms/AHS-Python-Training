{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Azure OpenAI Client\n",
    "\n",
    "1. **AzureOpenAI Client**:\n",
    "   - The `AzureOpenAI` client is initialized using the Azure OpenAI API key, version, and endpoint.\n",
    "   - This client allows us to interact with GPT-4 hosted on Azure for various tasks such as query rewriting, generating step-back queries, and query decomposition.\n",
    "2. **Prompts from LangChain**:\n",
    "   - `PromptTemplate` is used to structure the prompt format when communicating with the Azure LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Azure OpenAI Client\n",
    "\n",
    "1. **AzureOpenAI Client**:\n",
    "   - The `AzureOpenAI` client is initialized using the Azure OpenAI API key, version, and endpoint.\n",
    "   - This client allows us to interact with GPT-4 hosted on Azure for various tasks such as query rewriting, generating step-back queries, and query decomposition.\n",
    "2. **Prompts from LangChain**:\n",
    "   - `PromptTemplate` is used to structure the prompt format when communicating with the Azure LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and set up Azure OpenAI client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d69d52da8164fc8939c74eae4d66ef9\n",
      "2024-02-15-preview\n",
      "https://home-openai-01.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('variables.env')\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "print(azure_openai_api_key)\n",
    "print(azure_openai_api_version)\n",
    "print(azure_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Rewriting Function\n",
    "\n",
    "1. **LLM Call**:\n",
    "   - The `azure_llm_call()` function sends a prompt to GPT-4 using Azure OpenAI to generate a response.\n",
    "   - The `temperature=0` ensures deterministic output by limiting randomness in the response.\n",
    "\n",
    "2. **Query Rewriting Prompt**:\n",
    "   - The `query_rewrite_template` defines the structure of the prompt that will be used to ask GPT-4 to rewrite a query.\n",
    "   - The rewritten query should be more specific to improve retrieval quality in a RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM for query rewriting\n",
    "def azure_llm_call(prompt, model=\"gpt-4o\"):\n",
    "    response = azure_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create a prompt template for query rewriting\n",
    "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Rewritten query:\"\"\"\n",
    "\n",
    "# Define the prompt template for query rewriting\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")\n",
    "\n",
    "# Define the function to rewrite the query\n",
    "def rewrite_query(original_query):\n",
    "    prompt = query_rewrite_prompt.format(original_query=original_query)\n",
    "    return azure_llm_call(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Rewriting\n",
    "\n",
    "1. **Original Query**:\n",
    "   - A sample query is provided, asking about the impacts of climate change.\n",
    "\n",
    "2. **Rewriting Process**:\n",
    "   - The `rewrite_query()` function is applied to the original query, making it more specific.\n",
    "   - The results (original and rewritten queries) are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of climate change on the environment?\n",
      "\n",
      "Rewritten query: How does climate change affect various aspects of the environment, such as biodiversity, sea levels, weather patterns, and ecosystems?\n"
     ]
    }
   ],
   "source": [
    "# Example query over the \"Understanding Climate Change\" dataset\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "\n",
    "# Apply the query rewriting function to improve the query\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "\n",
    "# Print the original and rewritten queries\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nRewritten query:\", rewritten_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-back Prompting Function\n",
    "\n",
    "1. **Step-back Query**:\n",
    "   - A step-back query broadens the original question to retrieve more general context or background information, which can be useful in providing a comprehensive answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for step-back prompting\n",
    "step_back_template = \"\"\"You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Step-back query:\"\"\"\n",
    "\n",
    "# Define the prompt template for step-back prompting\n",
    "step_back_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=step_back_template\n",
    ")\n",
    "\n",
    "# Define the function to generate a step-back query\n",
    "def generate_step_back_query(original_query):\n",
    "    prompt = step_back_prompt.format(original_query=original_query)\n",
    "    return azure_llm_call(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Step-back Prompting\n",
    "\n",
    "1. **Step-back Query**:\n",
    "   - The original query is processed into a broader, more general query to provide a wider context.\n",
    "   - The function `generate_step_back_query()` is applied, and both the original and the step-back query are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of climate change on the environment?\n",
      "\n",
      "Step-back query: What are the general effects of climate change?\n"
     ]
    }
   ],
   "source": [
    "# Apply the step-back prompting function to generate a broader query\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "\n",
    "# Print the original and step-back queries\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nStep-back query:\", step_back_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-query Decomposition Function\n",
    "\n",
    "1. **Sub-query Decomposition**:\n",
    "   - This function breaks down a complex query into smaller sub-queries, which are easier to handle.\n",
    "   - For example, a question about the effects of climate change could be broken down into several sub-questions about different aspects, such as biodiversity, agriculture, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for sub-query decomposition\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\"\"\"\n",
    "\n",
    "# Define the prompt template for sub-query decomposition\n",
    "subquery_decomposition_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=subquery_decomposition_template\n",
    ")\n",
    "\n",
    "# Define the function to decompose a query into sub-queries\n",
    "def decompose_query(original_query: str):\n",
    "    prompt = subquery_decomposition_prompt.format(original_query=original_query)\n",
    "    response = azure_llm_call(prompt)\n",
    "    sub_queries = [q.strip() for q in response.split('\\n') if q.strip() and not q.strip().startswith('Sub-queries:')]\n",
    "    return sub_queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Sub-query Decomposition\n",
    "\n",
    "1. **Sub-query Generation**:\n",
    "   - The original query is broken down into several sub-queries using `decompose_query()`.\n",
    "   - These sub-queries are printed and can be used in the retrieval process to improve the response to the original question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sub-queries:\n",
      "1. Original query: What are the impacts of climate change on the environment?\n",
      "2. 1. How does climate change affect biodiversity and ecosystems?\n",
      "3. 2. What are the impacts of climate change on oceanic conditions and marine life?\n",
      "4. 3. How does climate change influence weather patterns and extreme weather events?\n",
      "5. 4. What are the effects of climate change on land use and agriculture?\n"
     ]
    }
   ],
   "source": [
    "# Decompose the original query into simpler sub-queries\n",
    "sub_queries = decompose_query(original_query)\n",
    "\n",
    "# Print the resulting sub-queries\n",
    "print(\"\\nSub-queries:\")\n",
    "for i, sub_query in enumerate(sub_queries, 1):\n",
    "    print(f\"{i}. {sub_query}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
