{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Overview:\n",
    "\n",
    "1. **AzureOpenAI**: Used to interact with Azure OpenAI for handling embeddings and chat completions.\n",
    "\n",
    "2. **FAISS**: A vector search engine that allows fast similarity search and retrieval based on embeddings.\n",
    "\n",
    "3. **LangChain Libraries**:\n",
    "   - **PyPDFLoader**: Loads PDF documents for processing.\n",
    "   - **RecursiveCharacterTextSplitter**: Splits large texts into smaller chunks, with overlap, to improve retrieval and context handling.\n",
    "   - **InMemoryDocstore**: Stores documents in memory, which is useful for quick retrieval in vector stores.\n",
    "   - **Document and BaseRetriever**: Define document structures and retrieval interfaces for LangChain workflows.\n",
    "   - **RetrievalQA**: A LangChain component for question-answering tasks that retrieves relevant documents and answers queries.\n",
    "\n",
    "4. **faiss**: Provides the ability to handle high-dimensional vector data for efficient document retrieval.\n",
    "\n",
    "5. **pydantic**: Handles data validation and model structures, especially when defining custom classes like `CustomAzureLLM`.\n",
    "\n",
    "6. **numpy**: For numerical operations, particularly in handling embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.language_models import LLM\n",
    "from openai import AzureOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import List, Any, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d69d52da8164fc8939c74eae4d66ef9\n",
      "2024-02-15-preview\n",
      "https://home-openai-01.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('variables.env')\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "\n",
    "print(azure_openai_api_key)\n",
    "print(azure_openai_api_version)\n",
    "print(azure_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomAzureEmbeddings Class:\n",
    "\n",
    "This class allows us to interact with Azure OpenAI's `text-embedding-ada-002` model to generate embeddings for both documents and queries.\n",
    "\n",
    "### Methods:\n",
    "1. **`__init__`**: Initializes the class by setting up the Azure OpenAI client using an API key, API version, and the Azure endpoint.\n",
    "  \n",
    "2. **`embed_documents`**: Loops through a list of documents and calls the `embed_query` method to generate embeddings for each document.\n",
    "\n",
    "3. **`embed_query`**: Sends a query or document to Azure OpenAIâ€™s embeddings model and retrieves the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAzureEmbeddings(Embeddings):\n",
    "    def __init__(self, api_key, api_version, azure_endpoint):\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=api_key,\n",
    "            api_version=api_version,\n",
    "            azure_endpoint=azure_endpoint\n",
    "        )\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomAzureLLM Class:\n",
    "\n",
    "This custom LLM class interacts with Azure OpenAI to handle chat completions using the GPT-4 model.\n",
    "\n",
    "### Components:\n",
    "1. **`__init__`**: Initializes the Azure OpenAI client for chat model interactions.\n",
    "\n",
    "2. **`_call`**: Sends a prompt to the Azure OpenAI GPT-4o model and retrieves the generated response.\n",
    "\n",
    "3. **`_llm_type`**: Specifies the type of the LLM being used (`\"custom_azure_llm\"`).\n",
    "\n",
    "4. **`_identifying_params`**: Returns the deployment name to identify the specific model being used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Custom Azure LLM class\n",
    "class CustomAzureLLM(LLM, BaseModel):\n",
    "    client: AzureOpenAI = Field(default=None)\n",
    "    deployment_name: str\n",
    "    api_key: str\n",
    "    api_version: str\n",
    "    azure_endpoint: str\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def __init__(self, **data):\n",
    "        super().__init__(**data)\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=self.api_key,\n",
    "            api_version=self.api_version,\n",
    "            azure_endpoint=self.azure_endpoint\n",
    "        )\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.deployment_name,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            stop=stop\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom_azure_llm\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"deployment_name\": self.deployment_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking Function:\n",
    "\n",
    "This function reranks a list of documents based on their relevance to the query using Azure OpenAI's GPT-4 model.\n",
    "\n",
    "### Steps:\n",
    "1. **`prompt_template`**: A prompt is sent to GPT-4o to rate the relevance of each document on a scale from 1 to 10.\n",
    "  \n",
    "2. **Loop Through Documents**: For each document, the relevance score is determined by GPT-4o based on the query.\n",
    "\n",
    "3. **Sort and Return Top Documents**: The documents are then sorted by their relevance score, and the top N documents are returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking function\n",
    "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
    "    client = AzureOpenAI(\n",
    "        api_key=azure_openai_api_key,\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_endpoint=azure_endpoint\n",
    "    )\n",
    "\n",
    "    prompt_template = \"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
    "    Query: {query}\n",
    "    Document: {doc}\n",
    "    Relevance Score:\"\"\"\n",
    "\n",
    "    scored_docs = []\n",
    "    for doc in docs:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that rates document relevance.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(query=query, doc=doc.page_content)}\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Replace with your actual GPT-4 deployment name\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        try:\n",
    "            score = float(result)\n",
    "        except ValueError:\n",
    "            score = 0  # Default score if parsing fails\n",
    "        scored_docs.append((doc, score))\n",
    "\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomRetriever Class:\n",
    "\n",
    "This class uses a FAISS vector store for initial document retrieval and then reranks the documents using the reranking function.\n",
    "\n",
    "### Components:\n",
    "1. **`vectorstore`**: The FAISS vector store that stores the document embeddings.\n",
    "\n",
    "2. **`get_relevant_documents`**: Retrieves documents using similarity search and reranks them based on relevance to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_6776\\3662707304.py:2: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class CustomRetriever(BaseRetriever, BaseModel):\n"
     ]
    }
   ],
   "source": [
    "# Custom Retriever class\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
    "        return rerank_documents(query, initial_docs, top_n=num_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode PDF and Create Vector Store:\n",
    "\n",
    "This function handles the following tasks:\n",
    "1. **Loads PDF**: Loads the text from the PDF file.\n",
    "2. **Text Splitting**: Splits the document into smaller chunks for embedding.\n",
    "3. **Generate Embeddings**: Embeds the text chunks using Azure OpenAI.\n",
    "4. **Create FAISS Index**: Stores the embeddings in a FAISS vector store for similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode PDF and create vector store\n",
    "def encode_pdf(path, chunk_size=300, chunk_overlap=200):\n",
    "\n",
    "\n",
    "    try:\n",
    "        embeddings_client = CustomAzureEmbeddings(\n",
    "            api_key=azure_openai_api_key,\n",
    "            api_version=azure_openai_api_version,\n",
    "            azure_endpoint=azure_endpoint\n",
    "        )\n",
    "\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        text_list = [doc.page_content for doc in texts]\n",
    "\n",
    "        embeddings = embeddings_client.embed_documents(text_list)\n",
    "        embeddings_array = np.array(embeddings)\n",
    "\n",
    "        dimension = embeddings_array.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings_array)\n",
    "\n",
    "        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(texts)})\n",
    "        index_to_docstore_id = {i: str(i) for i in range(len(texts))}\n",
    "\n",
    "        vectorstore = FAISS(\n",
    "            embedding_function=embeddings_client.embed_query,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id,\n",
    "        )\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution for Vector Store and QA System:\n",
    "\n",
    "This section runs the complete pipeline for document retrieval and question-answering.\n",
    "\n",
    "### Steps:\n",
    "1. **Vector Store Creation**: The PDF is processed and stored in a FAISS vector store.\n",
    "  \n",
    "2. **Custom Retriever**: A custom retriever is created to rerank and retrieve relevant documents.\n",
    "\n",
    "3. **Custom Azure LLM**: Azure GPT-4 is used to answer questions based on the retrieved context.\n",
    "\n",
    "4. **RetrievalQA Chain**: Combines document retrieval and question-answering into one workflow.\n",
    "  \n",
    "5. **Query Execution**: Answers the query and prints the results along with the source documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_6776\\2573306085.py:28: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the main impacts of climate change on biodiversity?\n",
      "\n",
      "Answer: The main impacts of climate change on biodiversity include disruptions to ecosystems, which can lead to the loss of species and habitats. Climate change can alter the distribution of species, affect migration patterns, and change the timing of biological events such as flowering and breeding. These changes can reduce biodiversity and weaken ecosystem resilience, making it harder for ecosystems to provide essential services and support human well-being.\n",
      "\n",
      "Source Documents:\n",
      "\n",
      "Document 1:\n",
      "the impacts of climate change and build a resilient, equitable, and thriving world for future \n",
      "generations. The journey ahead requires dedication, creativity, and c ollective effort from all \n",
      "sectors ...\n",
      "\n",
      "Document 2:\n",
      "goals. Policies should promote synergies between biodiversity conservation and climate \n",
      "action.  \n",
      "Chapter 10: Climate Change and Human Health  \n",
      "Health Impacts  \n",
      "Heat -Related Illnesses  \n",
      "Rising temper...\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    pdf_path = './data/Understanding_Climate_Change.pdf'\n",
    "    vectorstore = encode_pdf(pdf_path)\n",
    "\n",
    "    # Create the custom retriever\n",
    "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "\n",
    "    # Create the custom Azure LLM\n",
    "    azure_llm = CustomAzureLLM(\n",
    "        api_key=azure_openai_api_key,\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        deployment_name=\"gpt-4o\"  # Replace with your actual GPT-4 deployment name\n",
    "    )\n",
    "\n",
    "    # Create the RetrievalQA chain with the custom retriever and LLM\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=azure_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=custom_retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    # Example query\n",
    "    query = \"What are the main impacts of climate change on biodiversity?\"\n",
    "    result = qa_chain({\"query\": query})\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for i, doc in enumerate(result['source_documents']):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each source document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: retrieve_context_per_question\n",
    "\n",
    "This function is designed to retrieve the most relevant context (documents) for a given question from a vector store.\n",
    "\n",
    "### Parameters:\n",
    "- **`question`**: The query or question for which the context needs to be retrieved.\n",
    "- **`vectorstore`**: The FAISS vector store containing the document embeddings. This is where the relevant documents are searched from.\n",
    "- **`k`**: The number of top relevant documents to retrieve. The default value is 5.\n",
    "\n",
    "### Process:\n",
    "1. **Create a Retriever**: \n",
    "   - The vector store is converted into a retriever using the `as_retriever()` method.\n",
    "   - The number of documents retrieved is controlled by the `k` parameter.\n",
    "\n",
    "2. **Retrieve Relevant Documents**:\n",
    "   - The retriever searches for the most relevant documents for the given question.\n",
    "   - The `get_relevant_documents()` method returns the documents, and their content is extracted.\n",
    "\n",
    "3. **Prepare the Context**:\n",
    "   - The content of each retrieved document is stored in a list, which is returned by the function.\n",
    "\n",
    "### Output:\n",
    "- The function returns a list containing the contents (context) of the most relevant documents, which can be used to answer the given question or for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_per_question(question, vectorstore, k=5):\n",
    "    \"\"\"\n",
    "    Retrieves relevant context for a given question using the vector store retriever.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The question for which to retrieve relevant context.\n",
    "    - vectorstore: The FAISS vector store to retrieve the documents from.\n",
    "    - k (int): The number of top documents to retrieve (default is 5).\n",
    "    \n",
    "    Returns:\n",
    "    - context (list of str): A list of the retrieved documents' contents.\n",
    "    \"\"\"\n",
    "    # Create a retriever from the vector store\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    # Retrieve the relevant documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Prepare the context by extracting the content of each retrieved document\n",
    "    context = [doc.page_content for doc in docs]\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Querying the Vector Store and Printing Context:\n",
    "\n",
    "### Step 2: Query the Vector Store for Relevant Context\n",
    "1. **Define the Query**:\n",
    "   - In this step, you define the query (or question) for which you want to retrieve relevant documents from the vector store.\n",
    "   - The query in this case is `\"What is the main cause of climate change?\"`.\n",
    "\n",
    "2. **Retrieve Context**:\n",
    "   - The function `retrieve_context_per_question()` is called with the query and the `vectorstore` (the FAISS vector store created earlier).\n",
    "   - This function returns the top relevant documents from the vector store as context chunks. The number of chunks retrieved is determined by the `k` value in the function (default is 5).\n",
    "\n",
    "### Step 3: Print the Retrieved Context\n",
    "1. **Print Each Chunk**:\n",
    "   - The `for` loop iterates over each chunk of context returned by the `retrieve_context_per_question` function.\n",
    "   - For each chunk, it prints the content of the chunk along with its index (starting from 1).\n",
    "\n",
    "### Output:\n",
    "- This code outputs the content of the top N (k) most relevant documents (chunks) that are retrieved from the vector store based on the query.\n",
    "- The chunks represent different sections of documents that are relevant to the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_6776\\1784532726.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential\n",
      "Chunk 2: Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous\n",
      "Chunk 3: driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the\n",
      "Chunk 4: provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change\n",
      "Chunk 5: By understanding the causes, effects, and potential solutions to climate change, we can take \n",
      "informed actions to protect our planet for future generations. Global cooperation, innovation, \n",
      "and commitment are key to addressing this pressing challenge.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Query the vector store for relevant context\n",
    "query = \"What is the main cause of climate change?\"\n",
    "context = retrieve_context_per_question(query, vectorstore)\n",
    "\n",
    "# Step 3: Print the retrieved context\n",
    "for i, chunk in enumerate(context):\n",
    "    print(f\"Chunk {i + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: LLM based function to rerank the retrieved documents\n",
    "In this step, we define a method for reranking documents using an LLM (GPT-4) based approach. The function, `rerank_documents`, takes a query and a list of retrieved documents, asks the LLM to rate each document based on its relevance to the query, and then sorts the documents by their scores.\n",
    "\n",
    "This approach allows for better reranking by considering the context of the query rather than just keyword matches.\n",
    "#### Description: \n",
    "1. **Define the Query**:\n",
    "   - In this step, you define the query (or question) for which you want to retrieve relevant documents from the vector store.\n",
    "   - The query in this case is `\"What is the main cause of climate change?\"`.\n",
    "\n",
    "2. **Retrieve Context**:\n",
    "   - The function `retrieve_context_per_question()` is called with the query and the `vectorstore` (the FAISS vector store created earlier).\n",
    "   - This function returns the top relevant documents from the vector store as context chunks. The number of chunks retrieved is determined by the `k` value in the function (default is 5).\n",
    "\n",
    "1. **Print Each Chunk**:\n",
    "   - The `for` loop iterates over each chunk of context returned by the `retrieve_context_per_question` function.\n",
    "   - For each chunk, it prints the content of the chunk along with its index (starting from 1).\n",
    "\n",
    "### Output:\n",
    "- This code outputs the content of the top N (k) most relevant documents (chunks) that are retrieved from the vector store based on the query.\n",
    "- The chunks represent different sections of documents that are relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Example Usage of the Reranking Function\n",
    "In this step, we will demonstrate how to use the `rerank_documents` function with a sample query relevant to climate change. We first retrieve a set of documents using the `vectorstore.similarity_search` function and then rerank these documents based on relevance using our custom reranking method.\n",
    "\n",
    "After reranking, we print the top initial documents (before reranking) and the top reranked documents for comparison.\n",
    "#### Description:\n",
    "- The query asks, \"What are the impacts of climate change on biodiversity?\"\n",
    "- We retrieve 15 initial documents from the vector store using similarity search.\n",
    "\n",
    "- The rerank_documents function is then used to rerank these documents based on their relevance to the query.\n",
    "- We print the top 3 documents from the initial set (before reranking) and the top reranked documents to compare how the reranking process improved the relevance of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
    "    client = AzureOpenAI(\n",
    "        api_key=azure_openai_api_key,\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_endpoint=azure_endpoint\n",
    "    )\n",
    "\n",
    "    prompt_template = \"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches. Provide the numeric score followed by a brief explanation.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Document: {doc}\n",
    "\n",
    "Relevance Score (1-10) and brief explanation:\"\"\"\n",
    "\n",
    "    scored_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that rates document relevance.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(query=query, doc=doc.page_content)}\n",
    "        ]\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # Replace with your actual GPT-4 deployment name\n",
    "                messages=messages,\n",
    "                temperature=0\n",
    "            )\n",
    "            result = response.choices[0].message.content.strip()\n",
    "            print(f\"Document {i+1} raw response: {result}\")\n",
    "            \n",
    "            # Extract the numeric score using regex\n",
    "            score_match = re.search(r'Relevance Score:?\\s*(\\d+(?:\\.\\d+)?)', result)\n",
    "            if score_match:\n",
    "                score = float(score_match.group(1))\n",
    "                print(f\"Document {i+1} parsed score: {score}\")\n",
    "            else:\n",
    "                print(f\"Failed to parse score for Document {i+1}. Using default score.\")\n",
    "                score = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Document {i+1}: {str(e)}\")\n",
    "            score = 0\n",
    "        \n",
    "        scored_docs.append((doc, score))\n",
    "\n",
    "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nFinal Scores:\")\n",
    "    for i, (doc, score) in enumerate(reranked_docs):\n",
    "        print(f\"Document {i+1}: Score {score}\")\n",
    "    \n",
    "    return [doc for doc, _ in reranked_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 raw response: Relevance Score: 4\n",
      "\n",
      "Explanation: The document contains a chapter specifically titled \"Climate Change and Biodiversity,\" which suggests that it may cover the impacts of climate change on biodiversity. However, the provided excerpt does not include any specific information or details about these impacts. The general context of the document seems to focus on broader themes of resilience and collective effort, rather than directly addressing the query. Therefore, while there is some potential relevance, the excerpt itself does not provide concrete information on the impacts of climate change on biodiversity.\n",
      "Document 1 parsed score: 4.0\n",
      "Document 2 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document primarily focuses on the health impacts of climate change, specifically heat-related illnesses, rather than the impacts of climate change on biodiversity. While there is a brief mention of promoting synergies between biodiversity conservation and climate action, the main content does not address the query's focus on biodiversity.\n",
      "Document 2 parsed score: 2.0\n",
      "Document 3 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document is highly relevant to the query as it discusses the impacts of climate change on marine biodiversity, specifically mentioning rising sea temperatures, ocean acidification, and changing currents. These factors directly affect marine species and ecosystems, which aligns well with the query's focus on the impacts of climate change on biodiversity. However, the document is somewhat limited in scope as it only addresses marine ecosystems and does not cover terrestrial or freshwater biodiversity, which prevents it from scoring a perfect 10.\n",
      "Document 3 parsed score: 8.0\n",
      "Document 4 raw response: Relevance Score: 7\n",
      "\n",
      "Explanation: The document directly addresses the impacts of climate change on biodiversity, specifically within marine ecosystems. It mentions rising sea temperatures, ocean acidification, and changing currents, which are relevant factors affecting marine biodiversity. However, the document is somewhat incomplete and lacks broader context or details on other ecosystems and species, which limits its overall relevance to the query.\n",
      "Document 4 parsed score: 7.0\n",
      "Document 5 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document directly addresses the impacts of climate change on biodiversity by discussing how climate change is altering terrestrial ecosystems, shifting habitat ranges, changing species distributions, and impacting ecosystem functions. It mentions specific types of ecosystems (forests, grasslands, and deserts) and the resulting changes in plant and animal species composition, which are highly relevant to the query. However, the document is somewhat incomplete as it cuts off mid-sentence and does not provide a comprehensive overview of all potential impacts on biodiversity.\n",
      "Document 5 parsed score: 8.0\n",
      "\n",
      "Final Scores:\n",
      "Document 1: Score 8.0\n",
      "Document 2: Score 8.0\n",
      "Document 3: Score 7.0\n",
      "Document 4: Score 4.0\n",
      "Document 5: Score 2.0\n",
      "\n",
      "Query: What are the impacts of climate change on biodiversity?\n",
      "Top reranked documents:\n",
      "\n",
      "Document 1:\n",
      "Marine ecosystems are highly vulnerable to climate change. Rising sea temperatures, ocean \n",
      "acidification, and changing currents affect marine biodiversity, from coral reefs to deep -sea \n",
      "habitats. Spe...\n",
      "\n",
      "Document 2:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n",
      "\n",
      "Document 3:\n",
      "of biodiversity and disrupt ecological balance.  \n",
      "Marine Ecosystems  \n",
      "Marine ecosystems are highly vulnerable to climate change. Rising sea temperatures, ocean \n",
      "acidification, and changing currents af...\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "\n",
    "# Retrieve the initial set of documents using vector similarity search\n",
    "initial_docs = vectorstore.similarity_search(query, k=5)  # Using 5 documents for demonstration\n",
    "\n",
    "# Apply the final improved LLM-based reranking function to reorder the documents based on relevance\n",
    "reranked_docs = rerank_documents(query, initial_docs)\n",
    "\n",
    "# Print the top reranked documents\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print the first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a Custom Retriever Based on Reranking\n",
    "In this step, we implement a custom retriever that integrates our reranking function. This custom retriever will first perform an initial retrieval of documents using a vector store and then apply the LLM-based reranking function to refine the results before returning the most relevant documents.\n",
    "\n",
    "The custom retriever is then used in a `RetrievalQA` chain with GPT-4 for answering questions based on the reranked documents.\n",
    "\n",
    "#### Description:\n",
    "- The CustomRetriever class extends the base retriever and integrates the reranking logic. It performs an initial vector search and then reranks the documents based on relevance\n",
    "- The RetrievalQA chain is created using this custom retriever, allowing us to answer questions by retrieving and reranking the most relevant documents before passing them to GPT-4 for generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_6776\\4279284410.py:4: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class CustomRetriever(BaseRetriever, BaseModel):\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Custom Retriever class\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str, num_docs=2) -> List[Document]:\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=30)\n",
    "        return rerank_documents(query, initial_docs, top_n=num_docs)\n",
    "\n",
    "\n",
    "# Create the custom retriever\n",
    "custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Example Query with the Custom QA Chain\n",
    "In this final step, we test our `qa_chain` with a sample query. The query is passed to the chain, which uses the custom retriever to retrieve and rerank the documents based on relevance. The top relevant documents are then used by GPT-4 to generate an answer.\n",
    "\n",
    "We print the answer along with the first 200 characters of each relevant source document to understand how the reranked documents were used.\n",
    "\n",
    "#### Description:\n",
    "- This cell runs a sample query against the qa_chain and retrieves the top relevant documents using the custom reranking retriever.\n",
    "- The answer generated by GPT-4 is printed, followed by a list of the relevant source documents, giving insight into how the reranked documents contributed to the answer. This ensures that the reranking process is effective and relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 raw response: Relevance Score: 4\n",
      "\n",
      "Explanation: The document contains a chapter specifically titled \"Climate Change and Biodiversity,\" which suggests that it may cover the impacts of climate change on biodiversity. However, the provided excerpt does not include any specific information or details about these impacts. The general language about building a resilient and equitable world is not directly relevant to the query. Therefore, while the document might contain relevant information in the specified chapter, the excerpt itself does not provide enough context to be highly relevant.\n",
      "Document 1 parsed score: 4.0\n",
      "Document 2 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document primarily focuses on the health impacts of climate change, specifically heat-related illnesses, rather than the impacts of climate change on biodiversity. While there is a brief mention of policies promoting synergies between biodiversity conservation and climate action, the main content does not address the query's focus on biodiversity.\n",
      "Document 2 parsed score: 2.0\n",
      "Document 3 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document is highly relevant to the query as it specifically addresses the impacts of climate change on marine biodiversity. It discusses how rising sea temperatures, ocean acidification, and changing currents affect various marine species and ecosystems, which directly relates to the broader topic of biodiversity. However, it focuses solely on marine ecosystems and does not cover terrestrial or freshwater biodiversity, which slightly limits its overall relevance to the query.\n",
      "Document 3 parsed score: 8.0\n",
      "Document 4 raw response: Relevance Score: 7\n",
      "\n",
      "Explanation: The document is relevant to the query as it discusses the impacts of climate change on marine biodiversity, mentioning specific factors like rising sea temperatures, ocean acidification, and changing currents. However, it is somewhat limited in scope, focusing primarily on marine ecosystems and not providing a comprehensive overview of the impacts on biodiversity as a whole. More detailed information and broader coverage of different ecosystems would increase its relevance.\n",
      "Document 4 parsed score: 7.0\n",
      "Document 5 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document directly addresses the impacts of climate change on biodiversity by discussing how climate change is altering terrestrial ecosystems, shifting habitat ranges, changing species distributions, and impacting ecosystem functions. It mentions specific ecosystems like forests, grasslands, and deserts, and notes the changes in plant and animal species composition, which are relevant to the query. However, the document is somewhat incomplete as it cuts off mid-sentence and could provide more detailed examples or broader impacts to achieve a higher score.\n",
      "Document 5 parsed score: 8.0\n",
      "Document 6 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document discusses the impacts of climate change on the distribution and prevalence of vector-borne diseases, focusing on human health and the spread of diseases like malaria, dengue fever, and Lyme disease. While it does touch on the broader effects of climate change, it does not directly address the impacts on biodiversity, which is the specific focus of the query. The relevance is minimal because the document does not provide information on how climate change affects various species, ecosystems, or the overall diversity of life.\n",
      "Document 6 parsed score: 2.0\n",
      "Document 7 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document directly addresses the query by discussing the impacts of climate change on biodiversity, specifically within terrestrial ecosystems. It mentions how climate change is altering habitat ranges, species distributions, and ecosystem functions, which are all relevant to understanding the broader impacts on biodiversity. However, the document appears to be a fragment and lacks comprehensive details, which slightly reduces its relevance.\n",
      "Document 7 parsed score: 8.0\n",
      "Document 8 raw response: Relevance Score: 7\n",
      "\n",
      "Explanation: The document addresses the impacts of climate change on biodiversity by mentioning changes in precipitation patterns, temperature, and water flow, which can lead to habitat loss and reduced biodiversity. It also highlights the vulnerability of freshwater species like fish and amphibians. However, the document is somewhat fragmented and lacks a comprehensive discussion on the broader impacts of climate change on biodiversity. The mention of conservation strategies and protected areas is relevant but not fully elaborated.\n",
      "Document 8 parsed score: 7.0\n",
      "Document 9 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document briefly mentions the loss of biodiversity as an environmental cost of climate change, which is relevant to the query. However, it does not provide detailed information or specific impacts of climate change on biodiversity. The focus of the document seems to be broader, touching on economic opportunities and the need for integrated solutions, rather than specifically addressing the impacts on biodiversity.\n",
      "Document 9 parsed score: 3.0\n",
      "Document 10 raw response: Relevance Score: 7\n",
      "\n",
      "Explanation: The document is relevant to the query as it discusses the impacts of climate change on biodiversity, specifically mentioning shifts in plant and animal species composition and the potential loss of biodiversity. It also touches on marine ecosystems, which are a critical component of global biodiversity. However, the document is somewhat fragmented and lacks comprehensive detail, which slightly reduces its overall relevance.\n",
      "Document 10 parsed score: 7.0\n",
      "Document 11 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document primarily discusses the impacts of climate change on food production, water availability, and related aspects of nutrition and health. While these topics are related to the broader effects of climate change, the document does not directly address the specific query about the impacts of climate change on biodiversity. The connection to biodiversity is indirect and not explicitly covered, making the document only marginally relevant to the query.\n",
      "Document 11 parsed score: 3.0\n",
      "Document 12 raw response: Relevance Score: 6\n",
      "\n",
      "Explanation: The document touches on the relationship between biodiversity and climate policy, suggesting that incorporating biodiversity considerations can enhance the effectiveness of climate policies. It also mentions the role of protecting and restoring ecosystems in climate mitigation and adaptation. However, it does not directly address the specific impacts of climate change on biodiversity, which is the main focus of the query. The document is somewhat relevant but lacks detailed information on the direct effects of climate change on biodiversity.\n",
      "Document 12 parsed score: 6.0\n",
      "Document 13 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document briefly touches on climate change but focuses primarily on food production, water availability, and public health rather than directly addressing the impacts of climate change on biodiversity. While there is some indirect relevance, the document does not provide specific information on how climate change affects biodiversity, which is the main focus of the query.\n",
      "Document 13 parsed score: 3.0\n",
      "Document 14 raw response: Relevance Score: 6\n",
      "\n",
      "Explanation: The document mentions the impacts of activities like agriculture, logging, and mining on biodiversity in regions such as the Amazon, Congo Basin, and Southeast Asia. This is somewhat relevant to the query as it touches on habitat loss and species extinction, which are related to the broader impacts of climate change on biodiversity. However, the document does not explicitly discuss climate change itself or its direct impacts on biodiversity, which limits its relevance to the specific query.\n",
      "Document 14 parsed score: 6.0\n",
      "Document 15 raw response: Relevance Score: 4\n",
      "\n",
      "Explanation: The document mentions climate change and its link to extreme weather events, which can indirectly affect biodiversity. However, it does not specifically address the impacts of climate change on biodiversity itself. The focus is more on the general consequences of extreme weather events rather than on how these events specifically impact biodiversity.\n",
      "Document 15 parsed score: 4.0\n",
      "Document 16 raw response: Relevance Score: 7\n",
      "\n",
      "Explanation: The document discusses shifts in plant and animal species composition in various ecosystems (forests, grasslands, deserts) due to climate change, which can lead to a loss of biodiversity and disrupt ecological balance. This directly addresses the query about the impacts of climate change on biodiversity. However, the document is somewhat fragmented and lacks comprehensive detail, which prevents it from scoring higher.\n",
      "Document 16 parsed score: 7.0\n",
      "Document 17 raw response: Relevance Score: 6\n",
      "\n",
      "Explanation: The document mentions the loss of biodiversity as one of the environmental costs of climate change, which is directly relevant to the query. However, the primary focus of the document appears to be on social inequalities and the broader environmental impacts, rather than specifically detailing the impacts of climate change on biodiversity. Therefore, while it touches on the topic, it does not provide in-depth information specifically about biodiversity.\n",
      "Document 17 parsed score: 6.0\n",
      "Document 18 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document mentions biodiversity and climate change, but it primarily focuses on raising public awareness and education rather than directly addressing the impacts of climate change on biodiversity. While it is tangentially related, it does not provide specific information or detailed discussion on how climate change affects biodiversity.\n",
      "Document 18 parsed score: 3.0\n",
      "Document 19 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document mentions reduced biodiversity and habitat loss, which are relevant to the impacts of climate change on biodiversity. However, it lacks detailed information on how climate change specifically affects biodiversity. The focus seems to be more on conservation strategies and protected areas rather than directly addressing the query about the impacts of climate change.\n",
      "Document 19 parsed score: 3.0\n",
      "Document 20 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document mentions the importance of incorporating biodiversity considerations into climate policies and adaptive management practices, which are somewhat related to the impacts of climate change on biodiversity. However, it does not provide specific details or direct information about the actual impacts of climate change on biodiversity, making it only marginally relevant to the query.\n",
      "Document 20 parsed score: 3.0\n",
      "Document 21 raw response: Relevance Score: 4\n",
      "\n",
      "Explanation: The document mentions some effects of climate change, such as earlier springs, shorter and milder winters, and disruptions to plant and animal life cycles. However, it lacks depth and specificity regarding the broader impacts of climate change on biodiversity. It touches on relevant points but does not provide a comprehensive or detailed discussion on the topic.\n",
      "Document 21 parsed score: 4.0\n",
      "Document 22 raw response: Relevance Score: 4\n",
      "\n",
      "Explanation: The document touches on aspects related to climate change and biodiversity, such as identifying climate refugia and adaptive management practices. However, it does not directly address the specific impacts of climate change on biodiversity. Instead, it focuses more on conservation strategies and policy, which are related but not central to the query's intent of understanding the impacts.\n",
      "Document 22 parsed score: 4.0\n",
      "Document 23 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document mentions \"impacts of climate change,\" but it primarily focuses on the links between climate and health, technology development, and health data systems. It does not address the specific impacts of climate change on biodiversity, which is the main focus of the query. The mention of \"Climate Education\" and \"Education and Advocacy\" also does not provide relevant information about biodiversity.\n",
      "Document 23 parsed score: 2.0\n",
      "Document 24 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document primarily discusses the gendered impacts of climate change, focusing on how it affects men and women differently and exacerbates gender inequalities. While it is related to climate change, it does not address the specific query about the impacts of climate change on biodiversity. The connection to biodiversity is indirect and minimal, hence the low relevance score.\n",
      "Document 24 parsed score: 2.0\n",
      "Document 25 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document primarily discusses the economic costs of climate change, such as damage to infrastructure, reduced agricultural productivity, health care costs, and lost labor productivity. While it mentions extreme weather events, it does not address the specific impacts of climate change on biodiversity, which is the focus of the query. The relevance is very low because the document does not provide information on how climate change affects biodiversity.\n",
      "Document 25 parsed score: 2.0\n",
      "Document 26 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document mentions efforts related to biodiversity conservation, such as zoos, botanical gardens, and seed banks, which are somewhat relevant to the broader topic of biodiversity. However, it does not directly address the impacts of climate change on biodiversity. The mention of \"Integrating Biodiversity and Climate Action\" and \"Nature-Based Solutions\" hints at a potential connection, but the document lacks specific information on how climate change affects biodiversity, which is the core of the query.\n",
      "Document 26 parsed score: 3.0\n",
      "Document 27 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document briefly mentions climate change but focuses on the gendered impacts and the role of women in climate action. It does not address the specific query about the impacts of climate change on biodiversity. The content is more relevant to discussions on gender and climate change rather than biodiversity.\n",
      "Document 27 parsed score: 2.0\n",
      "Document 28 raw response: Relevance Score: 6\n",
      "\n",
      "Explanation: The document discusses how climate change is altering the timing and length of seasons, which indirectly affects ecosystems and plant and animal life cycles. While it touches on aspects that are relevant to the impacts of climate change on biodiversity, it does not provide detailed information specifically about biodiversity. The focus is more on seasonal changes rather than a comprehensive look at biodiversity impacts.\n",
      "Document 28 parsed score: 6.0\n",
      "Document 29 raw response: Relevance Score: 6\n",
      "\n",
      "Explanation: The document touches on the need for conservation strategies to account for climate change impacts, which is relevant to the query about the impacts of climate change on biodiversity. However, it does not provide specific details or examples of how climate change affects biodiversity directly. The focus is more on conservation strategies rather than the direct impacts on biodiversity.\n",
      "Document 29 parsed score: 6.0\n",
      "Document 30 raw response: Relevance Score: 7\n",
      "\n",
      "Explanation: The document appears to be a chapter specifically focused on \"Climate Change and Biodiversity,\" which directly aligns with the query. However, the provided excerpt is very brief and does not offer detailed information on the impacts of climate change on biodiversity. The mention of \"Impact on Ecosystems\" and \"Terrestrial Ecosystems\" suggests that the chapter likely contains relevant information, but the excerpt itself does not provide enough content to fully assess its relevance.\n",
      "Document 30 parsed score: 7.0\n",
      "\n",
      "Final Scores:\n",
      "Document 1: Score 8.0\n",
      "Document 2: Score 8.0\n",
      "Document 3: Score 8.0\n",
      "Document 4: Score 7.0\n",
      "Document 5: Score 7.0\n",
      "Document 6: Score 7.0\n",
      "Document 7: Score 7.0\n",
      "Document 8: Score 7.0\n",
      "Document 9: Score 6.0\n",
      "Document 10: Score 6.0\n",
      "Document 11: Score 6.0\n",
      "Document 12: Score 6.0\n",
      "Document 13: Score 6.0\n",
      "Document 14: Score 4.0\n",
      "Document 15: Score 4.0\n",
      "Document 16: Score 4.0\n",
      "Document 17: Score 4.0\n",
      "Document 18: Score 3.0\n",
      "Document 19: Score 3.0\n",
      "Document 20: Score 3.0\n",
      "Document 21: Score 3.0\n",
      "Document 22: Score 3.0\n",
      "Document 23: Score 3.0\n",
      "Document 24: Score 3.0\n",
      "Document 25: Score 2.0\n",
      "Document 26: Score 2.0\n",
      "Document 27: Score 2.0\n",
      "Document 28: Score 2.0\n",
      "Document 29: Score 2.0\n",
      "Document 30: Score 2.0\n",
      "\n",
      "Question: What are the impacts of climate change on biodiversity?\n",
      "Answer: Climate change impacts biodiversity by causing shifts in habitat ranges, altering species distributions, and affecting ecosystem functions. In marine ecosystems, rising sea temperatures, ocean acidification, and changing currents disrupt marine biodiversity, leading to species migration and changes in reproductive cycles, which can affect marine food webs and fisheries. In terrestrial ecosystems, climate change leads to shifts in plant and animal species composition in forests, grasslands, and deserts, potentially resulting in a loss of biodiversity.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "Marine ecosystems are highly vulnerable to climate change. Rising sea temperatures, ocean \n",
      "acidification, and changing currents affect marine biodiversity, from coral reefs to deep -sea \n",
      "habitats. Spe...\n",
      "\n",
      "Document 2:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n"
     ]
    }
   ],
   "source": [
    "# Run an example query\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the question and the generated answer\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "\n",
    "# Print the relevant source documents used to generate the answer\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print the first 200 characters of each document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Demonstrating the Importance of Reranking\n",
    "In this step, we create a small set of documents (chunks) with varying relevance to a query. These chunks contain similar statements about the capital of France, Paris. We will compare the results from a baseline vector search retrieval with the reranked results from our custom retriever.\n",
    "\n",
    "This example demonstrates how reranking can improve the relevance of the results by providing contextually better matches to the query.\n",
    "\n",
    "#### Description:\n",
    "- The dataset consists of simple statements and sentences about Paris and France. Some chunks provide more context about Paris being the capital, while others are less informative.\n",
    "- We run two retrieval methods: the baseline vector similarity search and the advanced reranked approach using our custom retriever.\n",
    "- The output compares the top 2 documents retrieved by each approach. The reranked results should show that contextually richer documents (those providing more than just \"the capital of France is...\") are prioritized over simpler, less informative ones. This demonstrates why reranking is valuable in a retrieval-augmented generation pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Document Chunks:\n",
    "\n",
    "1. **Chunks**: A set of sentences related to Paris and France, which simulate small sections of a document.\n",
    "2. **Convert to Document Objects**: Each text chunk is converted into a `Document` object from the LangChain framework to represent structured content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CustomAzureEmbeddings, CustomAzureLLM, and CustomRetriever are defined as in the previous example\n",
    "\n",
    "# Sample chunks containing different statements about Paris and France\n",
    "chunks = [\n",
    "    \"The capital of France is great.\",\n",
    "    \"The capital of France is huge.\",\n",
    "    \"The capital of France is beautiful.\",\n",
    "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
    "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\", \n",
    "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
    "]\n",
    "\n",
    "# Convert each chunk into a Document object\n",
    "docs = [Document(page_content=sentence) for sentence in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Baseline and Advanced Retrieval Techniques:\n",
    "\n",
    "1. **Azure OpenAI Configuration**:\n",
    "   - Configures the API key, version, and endpoint to interact with Azure OpenAI.\n",
    "  \n",
    "2. **CustomAzureEmbeddings**: \n",
    "   - Embeddings are generated using Azure OpenAI's `text-embedding-ada-002` model for each document.\n",
    "\n",
    "3. **Baseline Retrieval**:\n",
    "   - Using FAISS, the function performs a standard similarity search to find the top 2 documents that match the query.\n",
    "   - It prints the result of this baseline search.\n",
    "\n",
    "4. **Advanced Retrieval (Reranking)**:\n",
    "   - After the initial similarity search, documents are reranked based on their relevance to the query using a `CustomRetriever`.\n",
    "   - The function then prints the top reranked documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Techniques\n",
      "==================================\n",
      "Query: what is the capital of france?\n",
      "\n",
      "Baseline Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Advanced Retrieval Result:\n",
      "Document 1 raw response: Relevance Score: 3\n",
      "\n",
      "Explanation: The document does mention that the capital of France is \"great,\" which indirectly implies knowledge of the capital. However, it does not directly answer the query by stating that the capital of France is Paris. The response is vague and lacks the specific information requested.\n",
      "Document 1 parsed score: 3.0\n",
      "Document 2 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document directly addresses the query by stating that the capital of France is beautiful. While it does not explicitly name the capital, it implies knowledge of the capital, which is Paris. The document is relevant but could be more precise by explicitly stating \"The capital of France is Paris.\"\n",
      "Document 2 parsed score: 8.0\n",
      "Document 3 raw response: Relevance Score: 2\n",
      "\n",
      "Explanation: The document mentions the capital of France but does not provide the specific information requested, which is the name of the capital. The word \"huge\" is not relevant to the query.\n",
      "Document 3 parsed score: 2.0\n",
      "Document 4 raw response: Relevance Score: 6\n",
      "\n",
      "Explanation: The document mentions Paris and refers to it as a \"great capital city,\" which directly answers the query about the capital of France. However, the primary focus of the document is on the author's personal experience and enjoyment of the city, rather than providing a straightforward answer to the query. Therefore, while it is relevant, it is not the most direct or informative response.\n",
      "Document 4 parsed score: 6.0\n",
      "Document 5 raw response: Relevance Score: 8\n",
      "\n",
      "Explanation: The document indirectly answers the query by mentioning that Paris is the capital of France. While it does not explicitly state \"Paris is the capital of France,\" it provides enough context for a reader to infer this information. The additional details about Paris being a beautiful city and the mention of the Eiffel Tower add context but do not detract from the relevance to the query.\n",
      "Document 5 parsed score: 8.0\n",
      "\n",
      "Final Scores:\n",
      "Document 1: Score 8.0\n",
      "Document 2: Score 8.0\n",
      "Document 3: Score 6.0\n",
      "Document 4: Score 3.0\n",
      "Document 5: Score 2.0\n",
      "\n",
      "Document 1:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Document 2:\n",
      "Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
      "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to compare baseline and advanced (reranked) retrieval techniques\n",
    "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
    "\n",
    "\n",
    "    # Create Azure embeddings\n",
    "    embeddings = CustomAzureEmbeddings(\n",
    "        api_key=azure_openai_api_key,\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_endpoint=azure_endpoint\n",
    "    )\n",
    "\n",
    "    # Create vector store\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Baseline Retrieval\n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
    "    for i, doc in enumerate(baseline_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "    # Advanced Retrieval using Reranking\n",
    "    print(\"\\nAdvanced Retrieval Result:\")\n",
    "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
    "    advanced_docs = custom_retriever.get_relevant_documents(query)\n",
    "    for i, doc in enumerate(advanced_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Query to demonstrate reranking\n",
    "    query = \"what is the capital of france?\"\n",
    "    compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Cross Encoder models\n",
    "###  Cross Encoder-Based Document Reranking\n",
    "In this step, we use a pre-trained cross-encoder model from Hugging Face (`ms-marco-MiniLM-L-6-v2`) to rerank the retrieved documents. The cross-encoder model works by scoring document-query pairs to determine relevance.\n",
    "\n",
    "We define a `CrossEncoderRetriever` class that first retrieves documents using vector similarity search and then reranks them based on their cross-encoder scores.\n",
    "\n",
    "#### Description:\n",
    "- This cell implements a cross-encoder-based reranking retriever. It first retrieves an initial set of documents based on vector similarity search.\n",
    "- The retrieved documents are then paired with the query and passed through a pre-trained cross-encoder model, which assigns relevance scores to each document-query pair.\n",
    "- The documents are sorted based on their scores, and the top rerank_top_k documents are returned. This approach provides more context-aware document retrieval compared to simple similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_6776\\1698021441.py:6: DeprecationWarning: Retrievers must implement abstract `_get_relevant_documents` method instead of `get_relevant_documents`\n",
      "  class AzureCrossEncoderRetriever(BaseRetriever, BaseModel):\n",
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_6776\\1698021441.py:6: DeprecationWarning: Retrievers must implement abstract `_aget_relevant_documents` method instead of `aget_relevant_documents`\n",
      "  class AzureCrossEncoderRetriever(BaseRetriever, BaseModel):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Retrieval Techniques\n",
      "==================================\n",
      "Query: what is the capital of france?\n",
      "\n",
      "Baseline Retrieval Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n",
      "\n",
      "Cross-Encoder Reranking Result:\n",
      "\n",
      "Document 1:\n",
      "The capital of France is great.\n",
      "\n",
      "Document 2:\n",
      "The capital of France is beautiful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming CustomAzureEmbeddings and CustomAzureLLM are defined as before\n",
    "\n",
    "# Initialize the cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "class AzureCrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
    "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
    "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
    "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Perform an initial retrieval using vector similarity search\n",
    "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "        \n",
    "        # Create pairs of query and document content for cross-encoder reranking\n",
    "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
    "        \n",
    "        # Use the cross-encoder to predict relevance scores for each query-document pair\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        \n",
    "        # Sort the documents by their cross-encoder scores in descending order\n",
    "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return the top reranked documents based on the cross-encoder scores\n",
    "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")\n",
    "\n",
    "# Function to compare baseline and cross-encoder reranking retrieval techniques\n",
    "def compare_rag_techniques(query: str, docs: List[Document]) -> None:\n",
    "\n",
    "\n",
    "    embeddings = CustomAzureEmbeddings(\n",
    "        api_key=azure_openai_api_key,\n",
    "        api_version=azure_openai_api_version,\n",
    "        azure_endpoint=azure_endpoint\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    print(\"Comparison of Retrieval Techniques\")\n",
    "    print(\"==================================\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Baseline Retrieval\n",
    "    print(\"Baseline Retrieval Result:\")\n",
    "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
    "    for i, doc in enumerate(baseline_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "    # Cross-Encoder Reranking Retrieval\n",
    "    print(\"\\nCross-Encoder Reranking Result:\")\n",
    "    cross_encoder_retriever = AzureCrossEncoderRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        cross_encoder=cross_encoder,\n",
    "        k=5,\n",
    "        rerank_top_k=2\n",
    "    )\n",
    "    reranked_docs = cross_encoder_retriever.get_relevant_documents(query)\n",
    "    for i, doc in enumerate(reranked_docs):\n",
    "        print(f\"\\nDocument {i+1}:\")\n",
    "        print(doc.page_content)\n",
    "\n",
    "# Sample chunks\n",
    "chunks = [\n",
    "    \"The capital of France is great.\",\n",
    "    \"The capital of France is huge.\",\n",
    "    \"The capital of France is beautiful.\",\n",
    "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
    "    I really enjoyed all the cities in France, but its capital with the Eiffel Tower is my favorite city.\"\"\", \n",
    "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
    "]\n",
    "\n",
    "# Convert each chunk into a Document object\n",
    "docs = [Document(page_content=sentence) for sentence in chunks]\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"what is the capital of france?\"\n",
    "    compare_rag_techniques(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Example Query Using Cross-Encoder Retriever\n",
    "In this final step, we create an instance of the `CrossEncoderRetriever` and use it in a `RetrievalQA` chain to answer a query. The chain retrieves documents using a vector store, reranks them with the cross-encoder, and then uses GPT-4 to generate an answer. We print both the answer and the relevant source documents.\n",
    "#### Description:\n",
    "- This cell creates an instance of the CrossEncoderRetriever, which uses a cross-encoder model to rerank the retrieved documents.\n",
    "- The RetrievalQA chain is set up with GPT-4 (gpt-4o) as the LLM and uses the cross-encoder retriever to return the most relevant documents.\n",
    "- The query asks about the impacts of climate change on biodiversity. The answer is generated by GPT-4, and the top 5 relevant source documents (based on reranking) are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Cross-Encoder Model:\n",
    "\n",
    "- `CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')`: \n",
    "   - This model is used for reranking documents after they have been retrieved by the initial vector store search.\n",
    "   - The model scores each document based on how well it answers the query. \n",
    "   - `ms-marco-MiniLM-L-6-v2` is a commonly used model for such reranking tasks.\n",
    "\n",
    "## 2. Create the Cross-Encoder Retriever:\n",
    "\n",
    "- `AzureCrossEncoderRetriever`: \n",
    "   - Combines a standard retriever (vector store search) with the cross-encoder for reranking.\n",
    "   - First, it retrieves 10 documents (`k=10`) based on their vector similarity.\n",
    "   - It then reranks the top 10 documents using the cross-encoder and selects the top 5 most relevant ones (`rerank_top_k=5`).\n",
    "\n",
    "## 3. Set up the Azure LLM (GPT-4):\n",
    "\n",
    "- `CustomAzureLLM`: \n",
    "   - A custom class to interact with Azure OpenAI's GPT-4 model.\n",
    "   - The `api_key`, `api_version`, and `azure_endpoint` are specific to your Azure OpenAI resource.\n",
    "   - `deployment_name=\"gpt-4o\"`: Specifies the deployed GPT-4 model in your Azure OpenAI instance.\n",
    "\n",
    "## 4. Create the RetrievalQA Chain:\n",
    "\n",
    "- `RetrievalQA.from_chain_type`:\n",
    "   - Combines document retrieval with question-answering.\n",
    "   - The chain first uses the `cross_encoder_retriever` to find the most relevant documents, then passes those documents to `azure_llm` to generate the final answer.\n",
    "   - `return_source_documents=True`: Ensures the source documents used to generate the answer are returned, so the user can verify the answer's provenance.\n",
    "\n",
    "## 5. Example Query:\n",
    "\n",
    "- `query = \"What are the impacts of climate change on biodiversity?\"`: \n",
    "   - This query is used to test the system, where the retriever will find relevant documents, and GPT-4 will generate the final answer based on those documents.\n",
    "\n",
    "## 6. Display Results:\n",
    "\n",
    "- **Answer**: The output generated by GPT-4 based on the retrieved and reranked documents.\n",
    "- **Source Documents**: The most relevant source documents used to answer the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the impacts of climate change on biodiversity?\n",
      "Answer: Climate change impacts biodiversity by altering terrestrial and marine ecosystems. In terrestrial ecosystems, it shifts habitat ranges, changes species distributions, and impacts ecosystem functions, leading to shifts in plant and animal species composition. These changes can result in a loss of biodiversity and disrupt ecological balance. In marine ecosystems, rising sea temperatures and other climate-related changes make them highly vulnerable, further affecting biodiversity.\n",
      "\n",
      "Relevant source documents:\n",
      "\n",
      "Document 1:\n",
      "sectors of society.  \n",
      "Chapter 9: Climate Change and Biodiversity  \n",
      "Impact on Ecosystems  \n",
      "Terrestrial Ecosystems  \n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changin...\n",
      "\n",
      "Document 2:\n",
      "the impacts of climate change and build a resilient, equitable, and thriving world for future \n",
      "generations. The journey ahead requires dedication, creativity, and c ollective effort from all \n",
      "sectors ...\n",
      "\n",
      "Document 3:\n",
      "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
      "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
      "experiencing shi...\n",
      "\n",
      "Document 4:\n",
      "goals. Policies should promote synergies between biodiversity conservation and climate \n",
      "action.  \n",
      "Chapter 10: Climate Change and Human Health  \n",
      "Health Impacts  \n",
      "Heat -Related Illnesses  \n",
      "Rising temper...\n",
      "\n",
      "Document 5:\n",
      "experiencing shifts in plant and animal species composition. These changes  can lead to a loss \n",
      "of biodiversity and disrupt ecological balance.  \n",
      "Marine Ecosystems  \n",
      "Marine ecosystems are highly vulne...\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Assuming you have already defined CustomAzureLLM, AzureCrossEncoderRetriever, and have the vectorstore ready\n",
    "\n",
    "# Initialize the cross-encoder model\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# Create the cross-encoder retriever\n",
    "cross_encoder_retriever = AzureCrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    cross_encoder=cross_encoder,\n",
    "    k=10,  # Retrieve 10 documents initially\n",
    "    rerank_top_k=5  # Return top 5 after reranking\n",
    ")\n",
    "\n",
    "# Set up the Azure LLM (GPT-4)\n",
    "azure_llm = CustomAzureLLM(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    deployment_name=\"gpt-4o\"  # Replace with your actual GPT-4 deployment name\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain with the cross-encoder retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=azure_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=cross_encoder_retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Example query\n",
    "query = \"What are the impacts of climate change on biodiversity?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the query and answer\n",
    "print(f\"\\nQuestion: {query}\")\n",
    "print(f\"Answer: {result['result']}\")\n",
    "\n",
    "# Print the relevant source documents\n",
    "print(\"\\nRelevant source documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content[:200] + \"...\")  # Print the first 200 characters of each document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
