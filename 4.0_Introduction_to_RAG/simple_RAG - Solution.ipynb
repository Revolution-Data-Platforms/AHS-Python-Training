{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Import Libraries\n",
    "\n",
    "## Libraries Overview:\n",
    "\n",
    "1. **os**: A standard library that allows interaction with the operating system, such as environment variable access.\n",
    "\n",
    "2. **dotenv**: Used to load environment variables from a `.env` file. This is useful for managing sensitive information like API keys.\n",
    "\n",
    "3. **AzureOpenAI (from openai)**: This is the library used to interact with the Azure OpenAI service. It helps you communicate with the Azure-hosted OpenAI models, such as `GPT-4` and embeddings models.\n",
    "\n",
    "4. **langchain.document_loaders.PyPDFLoader**: A LangChain utility for loading PDF files into a format that can be used for further text processing.\n",
    "\n",
    "5. **langchain.text_splitter.RecursiveCharacterTextSplitter**: A LangChain utility for splitting large text documents into smaller chunks based on characters or words, with some overlap for better context during embeddings.\n",
    "\n",
    "6. **langchain.vectorstores.faiss.FAISS**: FAISS is a vector search engine used to store document embeddings and allows quick similarity-based retrieval.\n",
    "\n",
    "7. **langchain_core.embeddings.Embeddings**: Base class for embedding models in LangChain.\n",
    "\n",
    "8. **langchain.docstore.InMemoryDocstore**: Stores documents in memory for quick retrieval.\n",
    "\n",
    "9. **faiss**: A fast library for nearest neighbor search in high-dimensional vector spaces, crucial for creating vector stores.\n",
    "\n",
    "10. **numpy**: A scientific computing library used here for handling arrays and matrices, particularly embedding vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('variables.env')\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d69d52da8164fc8939c74eae4d66ef9\n",
      "2024-02-15-preview\n",
      "https://home-openai-01.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "print(azure_openai_api_key)\n",
    "print(azure_openai_api_version)\n",
    "print(azure_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Embeddings Class:\n",
    "\n",
    "- This custom class **`CustomAzureEmbeddings`** is used to embed documents and queries using Azure OpenAI's embedding model.\n",
    "\n",
    "### Class Structure:\n",
    "- **`__init__` Method**: \n",
    "   - Initializes the class and sets up the Azure OpenAI client by providing an API key, API version, and the Azure endpoint.\n",
    "   - The class interacts with Azure OpenAI's embeddings service to generate document embeddings.\n",
    "  \n",
    "- **`embed_documents` Method**: \n",
    "   - Takes a list of texts as input and returns embeddings for each text.\n",
    "  \n",
    "- **`embed_query` Method**:\n",
    "   - Embeds a single text (query) by calling Azure OpenAI’s `embeddings.create` method.\n",
    "   - Uses the `text-embedding-ada-002` model to generate embeddings.\n",
    "#### List comprehension\n",
    " is a concise way to create lists in programming, particularly in Python. It allows you to generate a new list by applying an expression to each element of an existing iterable (like a list or range), often including a condition or filtering mechanism.\n",
    " [expression for item in iterable if condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAzureEmbeddings(AzureOpenAIEmbeddings):\n",
    "    def __init__(self, api_key, api_version, azure_endpoint):\n",
    "        self.client = AzureOpenAIEmbeddings(\n",
    "            api_key=api_key,\n",
    "            api_version=api_version,\n",
    "            azure_endpoint=azure_endpoint\n",
    "        )\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('variables.env')\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode PDF and Create FAISS Vector Store:\n",
    "\n",
    "This function loads a PDF document, processes it into smaller chunks, embeds the chunks, and stores the embeddings in a FAISS index for efficient document retrieval.\n",
    "\n",
    "### Detailed Steps:\n",
    "1. **Azure OpenAI Configuration**:\n",
    "   - Defines API keys, version, and the endpoint.\n",
    "   \n",
    "2. **CustomAzureEmbeddings Initialization**:\n",
    "   - Initializes the `CustomAzureEmbeddings` class for interacting with Azure OpenAI’s embedding model.\n",
    "\n",
    "3. **Load and Process PDF**:\n",
    "   - Loads the PDF document using `PyPDFLoader`.\n",
    "   - Splits the document into smaller chunks (with overlapping sections) using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "4. **Generate Embeddings**:\n",
    "   - Embeds the text chunks using the custom embeddings class.\n",
    "   - Converts embeddings into a NumPy array for further processing.\n",
    "\n",
    "5. **Create FAISS Index**:\n",
    "   - Initializes a FAISS index with the dimensionality of the embeddings.\n",
    "   - Adds the embeddings to the index.\n",
    "\n",
    "6. **In-Memory Document Store**:\n",
    "   - Uses `InMemoryDocstore` to store the original documents and maps the document embeddings in the FAISS index to the corresponding document IDs.\n",
    "\n",
    "7. **Return Vector Store**:\n",
    "   - Returns the FAISS vector store, which can now be used for retrieval based on similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_pdf(path, chunk_size=300, chunk_overlap=200):\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Initialize the custom Azure OpenAI embeddings class\n",
    "        embeddings_client = AzureOpenAIEmbeddings(\n",
    "            api_key=azure_openai_api_key,\n",
    "            api_version=azure_openai_api_version,\n",
    "            azure_endpoint=azure_endpoint\n",
    "            \n",
    "        )\n",
    "\n",
    "        # Load and process the PDF\n",
    "        loader = PyPDFLoader(path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        text_list = [doc.page_content for doc in texts]\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = embeddings_client.embed_documents(text_list)\n",
    "        embeddings_array = np.array(embeddings)\n",
    "        print(f\"Document embeddings shape: {embeddings_array.shape}\")\n",
    "\n",
    "        # Create FAISS index\n",
    "        dimension = embeddings_array.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(embeddings_array)\n",
    "\n",
    "        print(f\"FAISS index dimension: {index.d}\")\n",
    "        print(f\"Number of vectors in FAISS index: {index.ntotal}\")\n",
    "\n",
    "        # Create InMemoryDocstore and index mapping\n",
    "        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(texts)})\n",
    "        index_to_docstore_id = {i: str(i) for i in range(len(texts))}\n",
    "\n",
    "        # Create FAISS vector store\n",
    "        vectorstore = FAISS(\n",
    "            embedding_function=embeddings_client.embed_query,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id,\n",
    "        )\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution for Vector Store Creation:\n",
    "\n",
    "This section defines the path to the PDF file and calls the `encode_pdf` function to create a FAISS vector store from the PDF content. If any errors occur during the process, they are caught and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings shape: (738, 1536)\n",
      "FAISS index dimension: 1536\n",
      "Number of vectors in FAISS index: 738\n",
      "Vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Path to your PDF\n",
    "pdf_path = './data/Understanding_Climate_Change.pdf'\n",
    "\n",
    "# Encode the PDF and create the vector store\n",
    "try:\n",
    "    vectorstore = encode_pdf(pdf_path)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during vector store creation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval:\n",
    "\n",
    "This part uses the FAISS vector store to retrieve the most relevant documents based on the user’s query.\n",
    "\n",
    "### Steps:\n",
    "1. **Create a Retriever**: \n",
    "   - A retriever is created from the vector store using `as_retriever()`.\n",
    "   - `search_kwargs={\"k\": 5}` specifies that the top 5 relevant documents should be retrieved.\n",
    "\n",
    "2. **Define Query**: \n",
    "   - The query asks about the main cause of climate change.\n",
    "\n",
    "3. **Retrieve Documents**:\n",
    "   - The retriever finds the top 5 relevant documents and concatenates their content into a `context`.\n",
    "   - If an error occurs during retrieval, it is caught and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_20456\\45327687.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved context:\n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential\n",
      "\n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous\n",
      "\n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the\n",
      "\n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change\n",
      "\n",
      "By understanding the causes, effects, and potential solutions to climate change, we can take \n",
      "informed actions to protect our planet for future generations. Global cooperation, innovation, \n",
      "and commitment are key to addressing this pressing challenge.\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Define a query\n",
    "query = \"What is the main cause of climate change?\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "try:\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    print(\"\\nRetrieved context:\")\n",
    "    print(context)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during document retrieval: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answer Using Azure OpenAI:\n",
    "\n",
    "This section calls Azure OpenAI’s `GPT-4` model to generate an answer to the user’s query based on the retrieved document context.\n",
    "\n",
    "### Steps:\n",
    "1. **Initialize Azure OpenAI Chat Client**: \n",
    "   - The chat client is initialized using the Azure OpenAI API key, version, and endpoint.\n",
    "\n",
    "2. **Create Chat Completion Request**: \n",
    "   - A completion request is made to Azure OpenAI, providing it with the retrieved context and user’s query.\n",
    "   - The system message sets the assistant's behavior as helpful.\n",
    "\n",
    "3. **Generate Answer**:\n",
    "   - The `gpt-4o` model generates an answer to the query based on the context.\n",
    "   - If an error occurs, it is caught and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The main cause of recent climate change is the increase in greenhouse gases in the atmosphere, primarily driven by human activities, particularly the emission of greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Azure OpenAI chat client\n",
    "chat_client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,  # Ensure this version supports chat completions\n",
    "    azure_endpoint= azure_endpoint\n",
    ")\n",
    "\n",
    "# Generate an answer using the chat model\n",
    "try:\n",
    "    response = chat_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Replace with your chat model deployment name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer the question based on the provided context.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during answer generation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bitpeak.com/chunking-methods-in-rag-methods-comparison/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
