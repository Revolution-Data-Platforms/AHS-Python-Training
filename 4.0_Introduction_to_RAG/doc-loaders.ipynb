{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to load CSVs\n",
    "LangChain offers a useful CSV loader that allows for reading and processing CSV files as documents. Each row of a CSV file is treated as an individual document, where each column is converted into a key-value pair. This setup allows the CSV data to be easily used in a retrieval-augmented generation (RAG) pipeline or similar applications.\n",
    "\n",
    "To load a CSV file in LangChain, you can use the CSVLoader from the langchain_community.document_loaders.csv_loader module. The CSVLoader automatically reads each row of the CSV file and outputs it as a document. Here‚Äôs an example of how to use it:\n",
    "\n",
    "\\\n",
    "https://python.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1\n",
      "Customer Id: DD37Cf93aecA6Dc\n",
      "First Name: Sheryl\n",
      "Last Name: Baxter\n",
      "Company: Rasmussen Group\n",
      "City: East Leonard\n",
      "Country: Chile\n",
      "Phone 1: 229.077.5154\n",
      "Phone 2: 397.884.0519x718\n",
      "Email: zunigavanessa@smith.info\n",
      "Subscription Date: 2020-08-24\n",
      "Website: http://www.stephenson.com/\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./data/customers-100.csv')\n",
    "data = loader.load()\n",
    "print(data[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load PDFs\n",
    "\n",
    "LangChain provides several ways to load and process PDF files, particularly through the use of the PyPDFLoader in the langchain_community.document_loaders module. This loader allows you to extract text from PDF documents and split them into chunks for further processing. Each page of the PDF can be treated as an individual document with associated metadata (e.g., page number, file source).\\\n",
    "https://python.langchain.com/docs/how_to/document_loader_pdf/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Using PyPDFLoader:\n",
    "1- Install Dependencies:\\\n",
    "You need to install the necessary libraries, including langchain_community and pypdf. You can do this using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-experimental 0.0.65 requires langchain-community<0.3.0,>=0.2.16, but you have langchain-community 0.3.0 which is incompatible.\n",
      "langchain-experimental 0.0.65 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 0.3.4 which is incompatible.\n",
      "llama-index-readers-file 0.2.1 requires pypdf<5.0.0,>=4.0.1, but you have pypdf 5.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain_community pypdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Split PDF:\n",
    "Use PyPDFLoader to load the PDF document, and split it into sections for analysis. Here‚Äôs a sample code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Climate Change  \n",
      "Chapter 1: Introduction to Climate Change  \n",
      "Climate change refers to significant, long -term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past cent ury, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change.  \n",
      "Historical Context  \n",
      "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
      "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
      "11,700 years ago marking the beginning of the modern climate era and  human civilization. \n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies f lourished, but the industrial era has seen \n",
      "unprecedented changes.  \n",
      "Modern Observations  \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhou se gases.  \n",
      "Chapter 2: Causes of Climate Change  \n",
      "Greenhouse Gases  \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is  essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate.  \n",
      "Fossil Fuels  \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today.  \n",
      "Coal\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./data/Understanding_Climate_Change.pdf')\n",
    "docs = loader.load_and_split()\n",
    "print(docs[0].page_content)  # Display the content of the first page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load HTML\n",
    "LangChain provides a variety of tools for loading and processing HTML documents, including the use of UnstructuredHTMLLoader and BSHTMLLoader to transform HTML content into usable text data.\\\n",
    "\n",
    "1- UnstructuredHTMLLoader: This loader is used to load and process HTML documents, extracting text content from web pages or other HTML files. Each document is converted into a LangChain document with metadata like the source URL or file path. For example:\\\n",
    "The resulting data contains the page content, and relevant metadata can be extracted from the HTML file.\\\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "loader = UnstructuredHTMLLoader('./data/azure openai.html')\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './data/azure openai.html'}, page_content=\"Accessibility Links\\n\\nSkip to main contentAccessibility help\\n\\nAccessibility feedback\\n\\nSearch Results\\n\\nHow to generate embeddings with Azure OpenAI Service\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ en-us ‚Ä∫ azure ‚Ä∫ ai-services\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ en-us ‚Ä∫ azure ‚Ä∫ ai-services\\n\\nAug 29, 2024 ‚Äî An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms.\\n\\nAzure OpenAI Service embeddings tutorial\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ Learn ‚Ä∫ Azure ‚Ä∫ AI Services\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ Learn ‚Ä∫ Azure ‚Ä∫ AI Services\\n\\nAug 30, 2024 ‚Äî Learn how to use Azure OpenAI's embeddings API for document search with the BillSum dataset.\\n\\nPrerequisites\\n\\nSet up\\n\\nAzure OpenAI - embeddings and cosine similarity\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ Learn ‚Ä∫ Azure ‚Ä∫ AI Services\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ Learn ‚Ä∫ Azure ‚Ä∫ AI Services\\n\\nSep 5, 2024 ‚Äî Learn more about how the Azure OpenAI embeddings API uses cosine similarity for document search and to measure similarity between texts.\\n\\nPeople also ask\\n\\nWhat is embedding in Azure OpenAI?\\n\\nWhat is embedding in OpenAI?\\n\\nWhat is the difference between Azure OpenAI and OpenAI?\\n\\nHow to use Azure OpenAI embeddings with langchain?\\n\\nFeedback\\n\\nAzureOpenAIEmbeddings | ü¶úÔ∏èüîó LangChain\\n\\nLangChain\\n\\nhttps://python.langchain.com ‚Ä∫ ... ‚Ä∫ Embedding models\\n\\nLangChain\\n\\nhttps://python.langchain.com ‚Ä∫ ... ‚Ä∫ Embedding models\\n\\nSetup\\u200b. To access AzureOpenAI embedding models you'll need to create an Azure account, get an API key, and install the langchain-openai integration package.\\n\\nHow to Generate Embeddings with Azure OpenAI?\\n\\nMedium ¬∑ The Tech Platform\\n\\n5 months ago\\n\\nMedium ¬∑ The Tech Platform\\n\\n5 months ago\\n\\nGenerating Embeddings with Azure OpenAI ¬∑ STEP 1: Importing Libraries ¬∑ STEP 2: Reading the CSV Data ¬∑ STEP 3: Selecting Relevant Columns ¬∑ STEP ...\\n\\nAzure embeddings example\\n\\nOpenAI Cookbook\\n\\nhttps://cookbook.openai.com ‚Ä∫ examples ‚Ä∫ embeddings\\n\\nOpenAI Cookbook\\n\\nhttps://cookbook.openai.com ‚Ä∫ examples ‚Ä∫ embeddings\\n\\nThis example will cover embeddings using the Azure OpenAI service. Setup First, we install the necessary dependencies and import the libraries we will be using.\\n\\nAzure OpenAI Embedding skill - Azure AI Search\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ en-us ‚Ä∫ cognitive-search-skil...\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ en-us ‚Ä∫ cognitive-search-skil...\\n\\nAug 28, 2024 ‚Äî The Azure OpenAI Embedding skill connects to a deployed embedding model on your Azure OpenAI resource to generate embeddings during indexing.\\n\\nAzure OpenAI Service models\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ azure ‚Ä∫ openai ‚Ä∫ concepts\\n\\nMicrosoft Learn\\n\\nhttps://learn.microsoft.com ‚Ä∫ azure ‚Ä∫ openai ‚Ä∫ concepts\\n\\nSep 12, 2024 ‚Äî A set of models that improve on GPT-3 and can understand and generate natural language and code. Embeddings, A set of models that can convert ...\\n\\nCompare Azure Government...\\n\\nModel retirements\\n\\nModel versions\\n\\nWhat's new\\n\\nText Embeddings - Azure OpenAI\\n\\nWeaviate\\n\\nhttps://weaviate.io ‚Ä∫ ... ‚Ä∫ Azure OpenAI\\n\\nWeaviate\\n\\nhttps://weaviate.io ‚Ä∫ ... ‚Ä∫ Azure OpenAI\\n\\nWeaviate's integration with Azure OpenAI's APIs allows you to access their models' capabilities directly from Weaviate.\\n\\nAzure OpenAI\\n\\nLlamaIndex\\n\\nhttps://docs.llamaindex.ai ‚Ä∫ stable ‚Ä∫ customization ‚Ä∫ llms\\n\\nLlamaIndex\\n\\nhttps://docs.llamaindex.ai ‚Ä∫ stable ‚Ä∫ customization ‚Ä∫ llms\\n\\nAzure openAI resources unfortunately differ from standard openAI resources as you can't generate embeddings unless you use an embedding model.\\n\\nPeople also ask\\n\\nFeedback\\n\\nPeople also search for\\n\\nAzure OpenAI embeddings langchain\\n\\nAzure OpenAI embeddings models\\n\\nAzure OpenAI embeddings example\\n\\nAzure OpenAI embeddings Python\\n\\nAzure OpenAI embeddings pricing\\n\\nAzure OpenAI embedding llama_index\\n\\nAzure OpenAI embeddings API\\n\\nAzure OpenAI embeddings langchain example\\n\\nPage Navigation\\n\\n1 2 3 4 5 6 7 8 9 10 Next\\n\\nAbout 170,000 results\\n\\nFilters and Topics\\n\\nAll\\n\\nVideos\\n\\nImages\\n\\nNews\\n\\nShopping\\n\\nMaps\\n\\nWeb\\n\\nFooter Links\\n\\nGoogle apps\\n\\nGoogle Account\\n\\nHadi Amiri\\n\\nmhadiamiri27@gmail.com\")]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TXT Files \n",
    "\n",
    "1. **`TextLoader`**:\n",
    "   - The `TextLoader` class from `langchain_community.document_loaders` is used to load text (.txt) files.\n",
    "   - It returns the content of the file wrapped in a `Document` object, which can be further used in LangChain for text processing or document retrieval tasks.\n",
    "   - The `.load()` method reads and loads the text into the Document format.\n",
    "\n",
    "2. **Demonstration**:\n",
    "   - A sample TXT file is loaded using the `TextLoader`. The content is printed, displaying the first 500 characters of the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT File Content Using LangChain Community Loader:\n",
      "FORM 10-K FORM 10-KUNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K \n",
      "(Mark One)\n",
      "‚òë ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "FOR THE FISCAL YEAR ENDED MAY 31, 2023 \n",
      "OR\n",
      "‚òêTRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "FOR THE TRANSITION PERIOD FROM TO .\n",
      "Commission File No. 1-10635 \n",
      "NIKE, Inc. \n",
      "(Exact name of Registrant as specified in its charter)\n",
      "Oregon 93-0584541\n",
      "(State or other j\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Function to load TXT files using langchain_community loader\n",
    "def load_txt_file_with_langchain(file_path):\n",
    "    \"\"\"\n",
    "    Load a TXT file and return its content using the langchain_community TextLoader.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the TXT file.\n",
    "    \n",
    "    Returns:\n",
    "        Document: A Document object containing the content of the TXT file.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Demonstrating loading a TXT file\n",
    "txt_file_path = './data/nike_2023_annual_report.txt'  # Replace with your TXT file path\n",
    "try:\n",
    "    txt_documents = load_txt_file_with_langchain(txt_file_path)\n",
    "    print(\"TXT File Content Using LangChain Community Loader:\")\n",
    "    for doc in txt_documents:\n",
    "        print(doc.page_content[:500])  # Printing the first 500 characters of each document\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the TXT file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading JSON Files\n",
    "\n",
    "1. **`JSONLoader`**:\n",
    "   - The `JSONLoader` class is used to load and parse JSON files into a Document object format.\n",
    "   - JSONLoader can handle JSON files, automatically converting them into documents that can be used in LangChain workflows for retrieval and analysis.\n",
    "\n",
    "## Handling Non-Text Content in JSON Files\n",
    "\n",
    "1. **`text_content=False`**:\n",
    "   - By default, `JSONLoader` expects `page_content` to be a string, but JSON content can often be a list or a dictionary.\n",
    "   - Setting `text_content=False` allows the loader to accept non-string content such as lists or dictionaries.\n",
    "\n",
    "2. **Converting Content to String**:\n",
    "   - Since `page_content` needs to be a string for `Document` objects in LangChain, we check if the content is a list or a dictionary.\n",
    "   - If the content is non-text (like a list or dict), we convert it to a string using `str()` before displaying or processing it.\n",
    "\n",
    "3. **Demonstration**:\n",
    "   - The sample JSON file is loaded with the updated loader.\n",
    "   - If the content is a list or dictionary, it is converted to a string for display, and the first 500 characters of each document are printed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON File Content Using LangChain Community Loader:\n",
      "[{'question': 'What does climate change refer to?', 'answer': 'Climate change refers to significant, long-term changes in the global climate.'}, {'question': \"What encompasses the planet's overall weather patterns?\", 'answer': \"The term 'global climate' encompasses the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period.\"}, {'question': 'What activities have significantly contributed to climate change over the past century?', 'answe\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Function to load JSON files using langchain_community loader\n",
    "def load_json_file_with_langchain(file_path, jq_schema):\n",
    "    \"\"\"\n",
    "    Load a JSON file and return its content using the langchain_community JSONLoader.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        jq_schema (str): JQ schema to extract specific fields from the JSON file.\n",
    "    \n",
    "    Returns:\n",
    "        Document: A Document object containing the content of the JSON file.\n",
    "    \"\"\"\n",
    "    # Set `text_content=False` to handle non-string content\n",
    "    loader = JSONLoader(file_path, jq_schema=jq_schema, text_content=False)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Converting list or dict content into string if necessary\n",
    "    for doc in documents:\n",
    "        if isinstance(doc.page_content, (list, dict)):\n",
    "            doc.page_content = str(doc.page_content)  # Convert list/dict to string for display purposes\n",
    "    return documents\n",
    "\n",
    "# Demonstrating loading a JSON file\n",
    "json_file_path = './data/q_a.json'  # Replace with your JSON file path\n",
    "jq_schema = '.'  # Replace with JQ schema ('.' extracts the entire content of the file)\n",
    "\n",
    "try:\n",
    "    json_documents = load_json_file_with_langchain(json_file_path, jq_schema)\n",
    "    print(\"JSON File Content Using LangChain Community Loader:\")\n",
    "    for doc in json_documents:\n",
    "        print(doc.page_content[:500])  # Printing the first 500 characters of each document\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the JSON file: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
