{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Chunking \n",
    "## Overview\n",
    "This code implements a semantic chunking approach for processing and retrieving information from PDF documents. Unlike traditional methods that split text based on fixed character or word counts, semantic chunking aims to create more meaningful and context-aware text segments.\n",
    "\n",
    "## Motivation\n",
    "Traditional text splitting methods often break documents at arbitrary points, potentially disrupting the flow of information and context. Semantic chunking addresses this issue by attempting to split text at more natural breakpoints, preserving semantic coherence within each chunk.\n",
    "\n",
    "## Key Components\n",
    "1-PDF processing and text extraction\\\n",
    "2-Semantic chunking using LangChain's SemanticChunker\\\n",
    "3-Vector store creation using FAISS and OpenAI embeddings\\\n",
    "4-Retriever setup for querying the processed documents\\\n",
    "## Method Details\n",
    "### Document Preprocessing\n",
    "1-The PDF is read and converted to a string using a custom read_pdf_to_string function.\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Load OpenAI API Key\n",
    "In this step, we import necessary libraries and load environment variables to access the OpenAI API. We append the parent directory to the Python path so we can access helper functions and evaluation modules.\n",
    "\n",
    "We also introduce a new type of text splitter called `SemanticChunker`, which will later be used to chunk the text based on meaning rather than fixed size. Finally, the OpenAI API key is securely loaded using the `.env` file.\n",
    "\\\n",
    "\\\n",
    "Description: This cell sets up the environment by importing necessary libraries for semantic chunking and evaluation. We use the SemanticChunker class to split text based on its meaning rather than fixed sizes. Additionally, we ensure that the OpenAI API key is loaded securely from the .env file for further use in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import Necessary Libraries\n",
    "This cell imports all required libraries for handling Azure OpenAI, document embeddings, vector storage, document loading, and processing.\n",
    "\n",
    "#### Explanation:\n",
    " This imports Azure OpenAI API, document embedding and retrieval libraries, FAISS for vector stores, and utilities for document processing and text splitting. PyMuPDF (fitz) is used for handling PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fitz  # PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('variables.env')\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(azure_openai_api_key)\n",
    "print(azure_openai_api_version)\n",
    "print(azure_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Configure Azure OpenAI API\n",
    "This cell sets up the Azure OpenAI client with your credentials (API key, endpoint, and API version).\n",
    "\n",
    "Explanation: Initializes the Azure OpenAI client with the required API key, endpoint, and version. Make sure to replace these with your actual Azure credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up the Azure OpenAI client\n",
    "# Hint: Use the AzureOpenAI class\n",
    "azure_client = # Your code here\n",
    "\n",
    "# Cell 5: Define Custom Embeddings Class for Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Define Custom Embeddings Class for Azure OpenAI\n",
    "This cell defines a custom class that interacts with Azure OpenAI to embed documents and queries using the embeddings model.\n",
    "\n",
    "Explanation: This class handles document embeddings by calling Azure OpenAI's embedding model (text-embedding-ada-002). It embeds both queries and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAzureEmbeddings(Embeddings):\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Read PDF and Extract Text\n",
    "This function extracts text from the PDF file using PyMuPDF.\n",
    "\n",
    "Explanation: This function reads a PDF file and extracts its text content, which will be later used for chunking and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Read PDF and Extract Text\n",
    "\n",
    "def read_pdf_to_string(path):\n",
    "    # TODO: Implement PDF text extraction using PyMuPDF (fitz)\n",
    "    # Hint: Open the PDF, iterate through pages, and extract text\n",
    "    doc = # Your code here\n",
    "    content = \"\"\n",
    "    # Your code here to extract text from each page\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Semantic Text Splitting\n",
    "This function splits large chunks of text into semantically meaningful sections using cosine similarity.\n",
    "\n",
    "Explanation: This function splits the text into chunks based on semantic similarity. It uses sentence embeddings and compares them using cosine similarity to create meaningful chunks for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Semantic Text Splitting\n",
    "\n",
    "def semantic_split(text, embeddings_client, max_chunk_size=1000, similarity_threshold=0.7):\n",
    "    # TODO: Split the text into sentences\n",
    "    # Hint: Use a list comprehension with text.split('.') to get sentences, and strip whitespace\n",
    "    sentences = # Your code here \n",
    "\n",
    "    # TODO: Generate embeddings for each sentence\n",
    "    # Hint: Use the embed_documents method of the embeddings_client\n",
    "    sentence_embeddings = # Your code here \n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = sentences[0]\n",
    "    current_embedding = sentence_embeddings[0]\n",
    "    \n",
    "    # Your code here to create semantic chunks\n",
    "    # Hint: Iterate through sentences and embeddings, comparing cosine similarity\n",
    "    # Use np.mean to update current_embedding when adding to a chunk\n",
    "    # Append to chunks when similarity is below threshold or max size is reached\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Encode PDF and Create FAISS Index\n",
    "This cell encodes the PDF, embeds the chunks, and creates a FAISS index.\n",
    "\n",
    "Explanation: This function reads the PDF, splits it semantically, embeds the chunks using Azure OpenAI, and stores them in a FAISS vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Encode PDF and Create FAISS Index\n",
    "\n",
    "def encode_pdf(path):\n",
    "    try:\n",
    "        embeddings_client = CustomAzureEmbeddings(azure_client)\n",
    "\n",
    "        # TODO: Read PDF content\n",
    "        # Hint: Use the read_pdf_to_string function you defined earlier\n",
    "        content = # Your code here\n",
    "\n",
    "        # TODO: Split content into semantic chunks\n",
    "        # Hint: Use the semantic_split function with the content and embeddings_client\n",
    "        chunks = # Your code here\n",
    "\n",
    "        # TODO: Create Document objects from chunks\n",
    "        # Hint: Use a list comprehension to create Document objects\n",
    "        texts = # Your code here \n",
    "\n",
    "        # TODO: Generate embeddings for the chunks\n",
    "        # Hint: Use embeddings_client.embed_documents() with a list comprehension\n",
    "        embeddings = # Your code here\n",
    "\n",
    "        # TODO: Convert embeddings to numpy array and print its shape\n",
    "        embeddings_array = # Your code here\n",
    "        print(f\"Document embeddings shape: {embeddings_array.shape}\")\n",
    "\n",
    "        # TODO: Get the dimension of the embeddings\n",
    "        # Hint: Use the shape attribute of the numpy array\n",
    "        dimension = # Your code here \n",
    "\n",
    "        # TODO: Create FAISS index\n",
    "        # Hint: Use FAISS.from_embeddings() with zipped texts and embeddings\n",
    "        index = # Your code here FAISS.from_embeddings(zip([t.page_content for t in texts], embeddings), embeddings_client)\n",
    "\n",
    "        print(f\"FAISS index contains {len(index.docstore._dict)} documents\")\n",
    "\n",
    "        return index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Execution for Vector Store Creation\n",
    "This cell handles the main execution for creating the vector store from the PDF.\n",
    "\n",
    "Explanation: Reads the PDF and encodes it using the previously defined functions. This stores the document embeddings in a FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Execution for Vector Store Creation\n",
    "\n",
    "pdf_path = \"./data/Understanding_Climate_Change.pdf\"\n",
    "\n",
    "# TODO: Create the vector store\n",
    "# Hint: Use the encode_pdf function and handle exceptions\n",
    "try:\n",
    "    vectorstore = # Your code here\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during vector store creation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieve Relevant Documents\n",
    "This cell creates a retriever using the FAISS vector store and retrieves relevant documents based on a query.\n",
    "\n",
    "Explanation: This part retrieves the most relevant documents from the FAISS index based on a user's query. It prints the context of the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Retrieve Relevant Documents\n",
    "\n",
    "# TODO: Create a retriever from the vector store\n",
    "# Hint: Use the as_retriever() method\n",
    "retriever = # Your code here\n",
    "\n",
    "query = \"What is the main cause of climate change?\"\n",
    "\n",
    "# TODO: Retrieve relevant documents\n",
    "# Hint: Use the get_relevant_documents() method and join the results.use .join()\n",
    "try:\n",
    "    docs = # Your code here\n",
    "    context = # Your code here\n",
    "    print(\"\\nRetrieved context:\")\n",
    "    print(context)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during document retrieval: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate an Answer Using Azure OpenAI GPT-4o\n",
    "This cell uses Azure OpenAI GPT-4o to generate an answer to the user's query based on the retrieved context.\n",
    "\n",
    "Explanation: This part generates an answer to the user's query by passing the retrieved context and the question to Azure OpenAI GPT-4o, which generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Generate an Answer Using Azure OpenAI GPT-4o\n",
    "\n",
    "# TODO: Generate an answer using the chat model\n",
    "# Hint: Use the azure_client object's chat.completions.create() method\n",
    "# You'll need to specify:\n",
    "# - The model to use (your GPT-4o deployment name)\n",
    "# - A list of messages, including a system message and a user message\n",
    "# - The user message should include the context and query\n",
    "\n",
    "try:\n",
    "    response = # Your code here\n",
    "    print(\"\\nAnswer:\")\n",
    "    # TODO: Print the generated answer\n",
    "    # Hint: The answer is in the 'content' of the first 'choice' message in the response\n",
    "    print(# Your code here)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during answer generation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
