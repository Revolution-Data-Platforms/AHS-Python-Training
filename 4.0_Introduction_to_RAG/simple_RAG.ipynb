{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install neccessary packages: \n",
    "\n",
    "os\\\n",
    "dotenv\\\n",
    "langchain_openai \n",
    "langchain_core.embeddings\\\n",
    "langchain.vectorstores\\\n",
    "langchain.docstore \\\n",
    "langchain.schema\\\n",
    "numpy (np)\\\n",
    "sklearn.metrics.pairwise\n",
    "\n",
    "PyMuPDF\\\n",
    "langchain.document_loaders\\\n",
    "langchain.text_splitter\\\n",
    "faiss\n",
    "\n",
    "example: !pip install os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Import Libraries\n",
    "\n",
    "## Libraries Overview:\n",
    "\n",
    "1. **os**: A standard library that allows interaction with the operating system, such as environment variable access.\n",
    "\n",
    "2. **dotenv**: Used to load environment variables from a `.env` file. This is useful for managing sensitive information like API keys.\n",
    "\n",
    "3. **AzureOpenAI**: This is the library used to interact with the Azure OpenAI service. It helps you communicate with the Azure-hosted OpenAI models, such as `GPT-4o` and embeddings models.\n",
    "\n",
    "4. **langchain.document_loaders.PyPDFLoader**: A LangChain utility for loading PDF files into a format that can be used for further text processing.\n",
    "\n",
    "5. **langchain.text_splitter.RecursiveCharacterTextSplitter**: A LangChain utility for splitting large text documents into smaller chunks based on characters or words, with some overlap for better context during embeddings.\n",
    "\n",
    "6. **langchain.vectorstores.faiss.FAISS**: FAISS is a vector search engine used to store document embeddings and allows quick similarity-based retrieval.\n",
    "\n",
    "7. **langchain_core.embeddings.Embeddings**: Base class for embedding models in LangChain.\n",
    "\n",
    "8. **langchain.docstore.InMemoryDocstore**: Stores documents in memory for quick retrieval.\n",
    "\n",
    "9. **faiss**: A fast library for nearest neighbor search in high-dimensional vector spaces, crucial for creating vector stores.\n",
    "\n",
    "10. **numpy**: A scientific computing library used here for handling arrays and matrices, particularly embedding vectors.\n",
    "\n",
    "11. **PyMuPDF** (also known as Fitz) is a Python binding for MuPDF, a lightweight PDF and XPS viewer. It allows users to extract text, images, and metadata from PDF files, as well as manipulate and annotate them\n",
    "\n",
    "12. **PyMuPDF**\n",
    "sklearn.metrics.pairwise is a module in scikit-learn that provides functions for evaluating pairwise distances or similarities between samples. It includes various distance metrics like cosine similarity, retrieval, and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "#add your\n",
    "# TODO: Load the Azure OpenAI configuration from environment variables\n",
    "# Hint: Use os.getenv() to get the API key, API version, and endpoint, feed the file to load_dotenv()\n",
    "load_dotenv()\n",
    "azure_openai_api_key = # Your code here\n",
    "azure_openai_api_version = # Your code here\n",
    "azure_endpoint = # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print the configuration to verify it's loaded correctly\n",
    "print()\n",
    "# Your code here\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Embeddings Class:\n",
    "\n",
    "- This custom class **`CustomAzureEmbeddings`** is used to embed documents and queries using Azure OpenAI's embedding model.\n",
    "\n",
    "### Class Structure:\n",
    "- **`__init__` Method**: \n",
    "   - Initializes the class and sets up the Azure OpenAI client by providing an API key, API version, and the Azure endpoint.\n",
    "   - The class interacts with Azure OpenAI's embeddings service to generate document embeddings.\n",
    "  \n",
    "- **`embed_documents` Method**: \n",
    "   - Takes a list of texts as input and returns embeddings for each text.\n",
    "  \n",
    "- **`embed_query` Method**:\n",
    "   - Embeds a single text (query) by calling Azure OpenAI’s `embeddings.create` method.\n",
    "   - Uses the `text-embedding-ada-002` model to generate embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Custom Embeddings Class\n",
    "\n",
    "class CustomAzureEmbeddings(Embeddings):\n",
    "    def __init__(self, ------, ------, ------):\n",
    "        # TODO: Initialize the Azure OpenAI client\n",
    "        # Hint: Use the AzureOpenAI class\n",
    "        self.client = # Your code here\n",
    "\n",
    "    def embed_documents(self, ------):\n",
    "        # TODO: Implement the method to embed multiple documents\n",
    "        # Hint: Use a list comprehension with self.embed_query\n",
    "        return # Your code here\n",
    "\n",
    "    def embed_query(self, ------):\n",
    "        # TODO: Implement the method to embed a single query\n",
    "        # Hint: Use self.client.embeddings.create(), use model=\"text-embedding-ada-002\"\n",
    "        response = # Your code here\n",
    "        return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode PDF and Create FAISS Vector Store:\n",
    "\n",
    "This function loads a PDF document, processes it into smaller chunks, embeds the chunks, and stores the embeddings in a FAISS index for efficient document retrieval.\n",
    "\n",
    "### Detailed Steps:\n",
    "1. **Azure OpenAI Configuration**:\n",
    "   - Defines API keys, version, and the endpoint.\n",
    "   \n",
    "2. **CustomAzureEmbeddings Initialization**:\n",
    "   - Initializes the `CustomAzureEmbeddings` class for interacting with Azure OpenAI’s embedding model.\n",
    "\n",
    "3. **Load and Process PDF**:\n",
    "   - Loads the PDF document using `PyPDFLoader`.\n",
    "   - Splits the document into smaller chunks (with overlapping sections) using `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "4. **Generate Embeddings**:\n",
    "   - Embeds the text chunks using the custom embeddings class.\n",
    "   - Converts embeddings into a NumPy array for further processing.\n",
    "\n",
    "5. **Create FAISS Index**:\n",
    "   - Initializes a FAISS index with the dimensionality of the embeddings.\n",
    "   - Adds the embeddings to the index.\n",
    "\n",
    "6. **In-Memory Document Store**:\n",
    "   - Uses `InMemoryDocstore` to store the original documents and maps the document embeddings in the FAISS index to the corresponding document IDs.\n",
    "\n",
    "7. **Return Vector Store**:\n",
    "   - Returns the FAISS vector store, which can now be used for retrieval based on similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Encode PDF Function\n",
    "\n",
    "def encode_pdf(path, chunk_size=300, chunk_overlap=200):\n",
    "    try:\n",
    "        # Initialize the custom Azure OpenAI embeddings class\n",
    "        embeddings_client = CustomAzureEmbeddings(\n",
    "            api_key=azure_openai_api_key,\n",
    "            api_version=azure_openai_api_version,\n",
    "            azure_endpoint=azure_endpoint\n",
    "        )\n",
    "\n",
    "        # TODO: Load and process the PDF\n",
    "        # Hint: Use PyPDFLoader and RecursiveCharacterTextSplitter\n",
    "        loader = # Your code here\n",
    "        documents = # Your code here\n",
    "        text_splitter = # Your code here\n",
    "        texts = # Your code here\n",
    "        text_list = [doc.page_content for doc in texts]\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = embeddings_client.embed_documents(text_list)\n",
    "        embeddings_array = np.array(embeddings)\n",
    "        print(f\"Document embeddings shape: {embeddings_array.shape}\")\n",
    "\n",
    "        # TODO: Create FAISS index\n",
    "        # Hint: Use faiss.IndexFlatL2\n",
    "        dimension = embeddings_array.shape[1]\n",
    "        index = # Your code here\n",
    "        index.add(embeddings_array)\n",
    "\n",
    "        print(f\"FAISS index dimension: {index.d}\")\n",
    "        print(f\"Number of vectors in FAISS index: {index.ntotal}\")\n",
    "\n",
    "        # Create InMemoryDocstore and index mapping\n",
    "        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(texts)})\n",
    "        index_to_docstore_id = {i: str(i) for i in range(len(texts))}\n",
    "\n",
    "        # TODO: Create FAISS vector store\n",
    "        # Hint: Use the FAISS class from langchain.vectorstores\n",
    "        vectorstore = # Your code here\n",
    "\n",
    "        return vectorstore\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution for Vector Store Creation:\n",
    "\n",
    "This section defines the path to the PDF file and calls the `encode_pdf` function to create a FAISS vector store from the PDF content. If any errors occur during the process, they are caught and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create Vector Store\n",
    "\n",
    "# Path to your PDF\n",
    "pdf_path = './data/Understanding_Climate_Change.pdf'\n",
    "\n",
    "# TODO: Encode the PDF and create the vector store\n",
    "# Hint: Use the encode_pdf function and handle exceptions\n",
    "try:\n",
    "    vectorstore = # Your code here\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during vector store creation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrieval:\n",
    "\n",
    "This part uses the FAISS vector store to retrieve the most relevant documents based on the user’s query.\n",
    "\n",
    "### Steps:\n",
    "1. **Create a Retriever**: \n",
    "   - A retriever is created from the vector store using `as_retriever()`.\n",
    "   - `search_kwargs={\"k\": 5}` specifies that the top 5 relevant documents should be retrieved.\n",
    "\n",
    "2. **Define Query**: \n",
    "   - The query asks about the main cause of climate change.\n",
    "\n",
    "3. **Retrieve Documents**:\n",
    "   - The retriever finds the top 5 relevant documents and concatenates their content into a `context`.\n",
    "   - If an error occurs during retrieval, it is caught and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Document Retrieval\n",
    "\n",
    "# TODO: Create a retriever from the vector store\n",
    "# Hint: Use the as_retriever() method\n",
    "retriever = # Your code here\n",
    "\n",
    "# Define a query\n",
    "query = \"What is the main cause of climate change?\"\n",
    "\n",
    "# TODO: Retrieve relevant documents\n",
    "# Hint: Use the get_relevant_documents() method and join the results\n",
    "try:\n",
    "    docs = # Your code here\n",
    "    # TODO: Create context by joining the content of retrieved documents\n",
    "    # Hint: Use a list comprehension to extract page_content and join with \"\\n\\n\"\n",
    "    context = # Your code here\n",
    "    print(\"\\nRetrieved context:\")\n",
    "    print(context)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during document retrieval: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answer Using Azure OpenAI:\n",
    "\n",
    "This section calls Azure OpenAI’s `GPT-4o` model to generate an answer to the user’s query based on the retrieved document context.\n",
    "\n",
    "### Steps:\n",
    "1. **Initialize Azure OpenAI Chat Client**: \n",
    "   - The chat client is initialized using the Azure OpenAI API key, version, and endpoint.\n",
    "\n",
    "2. **Create Chat Completion Request**: \n",
    "   - A completion request is made to Azure OpenAI, providing it with the retrieved context and user’s query.\n",
    "   - The system message sets the assistant's behavior as helpful.\n",
    "\n",
    "3. **Generate Answer**:\n",
    "   - The `gpt-4o` model generates an answer to the query based on the context.\n",
    "   - If an error occurs, it is caught and printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate Answer Using Azure OpenAI\n",
    "\n",
    "# TODO: Initialize the Azure OpenAI chat client\n",
    "# Hint: Use the AzureOpenAI class\n",
    "chat_client = # Your code here\n",
    "\n",
    "# TODO: Generate an answer using the chat model\n",
    "# Hint: Use chat_client.chat.completions.create()\n",
    "try:\n",
    "    response = # Your code here\n",
    "    print(\"\\nAnswer:\")\n",
    "    # Hint: Access the content of the first choice's message in the response\n",
    "    print(# Your code here)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during answer generation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
