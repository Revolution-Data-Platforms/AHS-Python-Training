{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Chunking \n",
    "## Overview\n",
    "This code implements a semantic chunking approach for processing and retrieving information from PDF documents. Unlike traditional methods that split text based on fixed character or word counts, semantic chunking aims to create more meaningful and context-aware text segments.\n",
    "\n",
    "## Motivation\n",
    "Traditional text splitting methods often break documents at arbitrary points, potentially disrupting the flow of information and context. Semantic chunking addresses this issue by attempting to split text at more natural breakpoints, preserving semantic coherence within each chunk.\n",
    "\n",
    "## Key Components\n",
    "1-PDF processing and text extraction\\\n",
    "2-Semantic chunking using LangChain's SemanticChunker\\\n",
    "3-Vector store creation using FAISS and OpenAI embeddings\\\n",
    "4-Retriever setup for querying the processed documents\\\n",
    "## Method Details\n",
    "### Document Preprocessing\n",
    "1-The PDF is read and converted to a string using a custom read_pdf_to_string function.\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Load OpenAI API Key\n",
    "In this step, we import necessary libraries and load environment variables to access the OpenAI API. We append the parent directory to the Python path so we can access helper functions and evaluation modules.\n",
    "\n",
    "We also introduce a new type of text splitter called `SemanticChunker`, which will later be used to chunk the text based on meaning rather than fixed size. Finally, the OpenAI API key is securely loaded using the `.env` file.\n",
    "\\\n",
    "\\\n",
    "Description: This cell sets up the environment by importing necessary libraries for semantic chunking and evaluation. We use the SemanticChunker class to split text based on its meaning rather than fixed sizes. Additionally, we ensure that the OpenAI API key is loaded securely from the .env file for further use in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Import Necessary Libraries\n",
    "This cell imports all required libraries for handling Azure OpenAI, document embeddings, vector storage, document loading, and processing.\n",
    "\n",
    "#### Explanation:\n",
    " This imports Azure OpenAI API, document embedding and retrieval libraries, FAISS for vector stores, and utilities for document processing and text splitting. PyMuPDF (fitz) is used for handling PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAI\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import fitz  # PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('variables.env')\n",
    "# Azure OpenAI configuration\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d69d52da8164fc8939c74eae4d66ef9\n",
      "2024-02-15-preview\n",
      "https://home-openai-01.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "print(azure_openai_api_key)\n",
    "print(azure_openai_api_version)\n",
    "print(azure_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Configure Azure OpenAI API\n",
    "This cell sets up the Azure OpenAI client with your credentials (API key, endpoint, and API version).\n",
    "\n",
    "Explanation: Initializes the Azure OpenAI client with the required API key, endpoint, and version. Make sure to replace these with your actual Azure credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: Define Custom Embeddings Class for Azure OpenAI\n",
    "This cell defines a custom class that interacts with Azure OpenAI to embed documents and queries using the embeddings model.\n",
    "\n",
    "Explanation: This class handles document embeddings by calling Azure OpenAI's embedding model (text-embedding-ada-002). It embeds both queries and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAzureEmbeddings(Embeddings):\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 4: Read PDF and Extract Text\n",
    "This function extracts text from the PDF file using PyMuPDF.\n",
    "\n",
    "Explanation: This function reads a PDF file and extracts its text content, which will be later used for chunking and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_to_string(path):\n",
    "    doc = fitz.open(path)\n",
    "    content = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        content += page.get_text()\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Semantic Text Splitting\n",
    "This function splits large chunks of text into semantically meaningful sections using cosine similarity.\n",
    "\n",
    "Explanation: This function splits the text into chunks based on semantic similarity. It uses sentence embeddings and compares them using cosine similarity to create meaningful chunks for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_split(text, embeddings_client, max_chunk_size=1000, similarity_threshold=0.7):\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    sentence_embeddings = embeddings_client.embed_documents(sentences)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = sentences[0]\n",
    "    current_embedding = sentence_embeddings[0]\n",
    "    \n",
    "    for sentence, embedding in zip(sentences[1:], sentence_embeddings[1:]):\n",
    "        similarity = cosine_similarity([current_embedding], [embedding])[0][0]\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_size and similarity > similarity_threshold:\n",
    "            current_chunk += '. ' + sentence\n",
    "            current_embedding = np.mean([current_embedding, embedding], axis=0)\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "            current_embedding = embedding\n",
    "    \n",
    "    chunks.append(current_chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Encode PDF and Create FAISS Index\n",
    "This cell encodes the PDF, embeds the chunks, and creates a FAISS index.\n",
    "\n",
    "Explanation: This function reads the PDF, splits it semantically, embeds the chunks using Azure OpenAI, and stores them in a FAISS vector store for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path):\n",
    "\n",
    "\n",
    "    try:\n",
    "        embeddings_client = CustomAzureEmbeddings(azure_client)\n",
    "\n",
    "        content = read_pdf_to_string(path)\n",
    "        chunks = semantic_split(content, embeddings_client)\n",
    "        texts = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "        embeddings = embeddings_client.embed_documents([t.page_content for t in texts])\n",
    "        embeddings_array = np.array(embeddings)\n",
    "        print(f\"Document embeddings shape: {embeddings_array.shape}\")\n",
    "\n",
    "        dimension = embeddings_array.shape[1]\n",
    "        index = FAISS.from_embeddings(zip([t.page_content for t in texts], embeddings), embeddings_client)\n",
    "\n",
    "        print(f\"FAISS index contains {len(index.docstore._dict)} documents\")\n",
    "\n",
    "        return index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 7: Main Execution for Vector Store Creation\n",
    "This cell handles the main execution for creating the vector store from the PDF.\n",
    "\n",
    "Explanation: Reads the PDF and encodes it using the previously defined functions. This stores the document embeddings in a FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings shape: (78, 1536)\n",
      "FAISS index contains 78 documents\n",
      "Vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "pdf_path = \"./data/Understanding_Climate_Change.pdf\"\n",
    "\n",
    "try:\n",
    "    vectorstore = encode_pdf(pdf_path)\n",
    "    print(\"Vector store created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during vector store creation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Retrieve Relevant Documents\n",
    "This cell creates a retriever using the FAISS vector store and retrieves relevant documents based on a query.\n",
    "\n",
    "Explanation: This part retrieves the most relevant documents from the FAISS index based on a user's query. It prints the context of the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HadiAmiri\\AppData\\Local\\Temp\\ipykernel_26648\\2887080802.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved context:\n",
      "During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies flourished, but the industrial era has seen \n",
      "unprecedented changes. Modern Observations \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhouse gases. Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect\n",
      "\n",
      "Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change. Historical Context \n",
      "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
      "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
      "11,700 years ago marking the beginning of the modern climate era and human civilization. Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives\n",
      "\n",
      "Chapter 3: Effects of Climate Change \n",
      "The effects of climate change are already being felt around the world and are projected to \n",
      "intensify in the coming decades. These effects include: \n",
      "Rising Temperatures \n",
      "Global temperatures have risen by about 1. 2 degrees Celsius (2. 2 degrees Fahrenheit) since \n",
      "the late 19th century. This warming is not uniform, with some regions experiencing more \n",
      "significant increases than others. Heatwaves \n",
      "Heatwaves are becoming more frequent and severe, posing risks to human health, agriculture, \n",
      "and infrastructure. Cities are particularly vulnerable due to the \"urban heat island\" effect. Heatwaves can lead to heat-related illnesses and exacerbate existing health conditions. Changing Seasons \n",
      "Climate change is altering the timing and length of seasons, affecting ecosystems and human \n",
      "activities. For example, spring is arriving earlier, and winters are becoming shorter and \n",
      "milder in many regions\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Define a query\n",
    "query = \"What is the main cause of climate change?\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "try:\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    print(\"\\nRetrieved context:\")\n",
    "    print(context)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during document retrieval: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Generate an Answer Using Azure OpenAI GPT-4o\n",
    "This cell uses Azure OpenAI GPT-4o to generate an answer to the user's query based on the retrieved context.\n",
    "\n",
    "Explanation: This part generates an answer to the user's query by passing the retrieved context and the question to Azure OpenAI GPT-4o, which generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The main cause of recent climate change, as outlined in the provided context, is the increase in greenhouse gases in the atmosphere. Human activities, particularly the emission of greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), are the primary drivers of this increase.\n"
     ]
    }
   ],
   "source": [
    "# Generate an answer using the chat model\n",
    "try:\n",
    "    response = azure_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Replace with your chat model deployment name\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer the question based on the provided context.\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during answer generation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
