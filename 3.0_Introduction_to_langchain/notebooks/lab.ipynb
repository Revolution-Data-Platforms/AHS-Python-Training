{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--------------------------------------\n",
    "Importing required classes/methods\n",
    "--------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "import pdfplumber\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAI, AzureOpenAIEmbeddings\n",
    "import faiss\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from pathlib import Path\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import re\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--------------------------------------\n",
    "Document Transformation\n",
    "--------------------------------------\n",
    "\"\"\"\n",
    "def document_transformation_pdf(file_path):\n",
    "    \"\"\"\n",
    "    #To load sing pdf file\n",
    "    # Step 1: Open the PDF using pdfplumber to extract metadata\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        metadata = pdf.metadata  # Extract metadata like title, author, etc.\n",
    "        print(\"PDF Metadata:\", metadata)\n",
    "    \n",
    "    # Step 2: Load the PDF content using LangChain's PDFPlumberLoader\n",
    "    loader = PDFPlumberLoader(file_path)\n",
    "    pdf_documents = loader.load()\n",
    "    for doc in pdf_documents:\n",
    "        # Add metadata to each document indicating its source\n",
    "        doc.metadata = {\n",
    "            \"title\": metadata.get(\"Title\", \"Unknown Title\"),\n",
    "            \"author\": metadata.get(\"Author\", \"Unknown Author\"),\n",
    "            \"subject\": metadata.get(\"Subject\", \"Unknown Subject\"),\n",
    "            \"creation_date\": metadata.get(\"CreationDate\", \"Unknown Date\"),\n",
    "        }\n",
    "    \"\"\"\n",
    "    #Step 2: Load Multiple pDF Documents\n",
    "    # Ensure the books directory exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"The directory {file_path} does not exist. Please check the path.\"\n",
    "        )\n",
    "\n",
    "    # List all text files in the directory\n",
    "    pdf_files = [f for f in os.listdir(file_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "    # Read the text content from each file and store it with metadata\n",
    "    pdf_documents = []\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(file_path, pdf_file)\n",
    "        loader = PDFPlumberLoader(file_path)\n",
    "        pdf_docs = loader.load()\n",
    "      \n",
    "        for doc in pdf_docs:\n",
    "            # Add metadata to each document indicating its source\n",
    "            doc.metadata = {\"source\": pdf_file}\n",
    "            pdf_documents.append(doc)\n",
    "    \n",
    "\n",
    "    # Step 3: Apply a text splitter for complex PDF processing\n",
    "    splitter = text_splitter(document=pdf_documents, chunk_size= 1000,chunk_overlap=2, option=0)\n",
    "    split_docs = splitter.split_documents(pdf_documents)\n",
    "    return split_docs\n",
    "\n",
    "def document_transformation_text(directory_path, chunk_size=1000, chunk_overlap=100, splitter_option=0):\n",
    "    \"\"\"\n",
    "    Transforms the text documents in the specified directory by applying a text splitter and returning split documents.\n",
    "    \n",
    "    :param directory_path: Path to the directory containing text files\n",
    "    :param chunk_size: Size of each chunk\n",
    "    :param chunk_overlap: Overlap between chunks\n",
    "    :param splitter_option: The text splitter to use. 0 for RecursiveCharacter, 1 for Character, 2 for Token\n",
    "    :return: List of split documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        raise FileNotFoundError(f\"The directory {directory_path} does not exist. Please check the path.\")\n",
    "    \n",
    "    # Step 2: List all text files in the directory\n",
    "    text_files = [f for f in os.listdir(directory_path) if f.endswith(\".txt\")]\n",
    "\n",
    "    # Step 3: Read the text content from each file\n",
    "    documents = []\n",
    "    for text_file in text_files:\n",
    "        file_path = os.path.join(directory_path, text_file)\n",
    "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "        text_docs = loader.load()\n",
    "\n",
    "        # Add each document to the documents list\n",
    "        documents.extend(text_docs)\n",
    "    \n",
    "    # Step 4: Apply the selected text splitter\n",
    "    splitter = text_splitter(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap, option=splitter_option)\n",
    "\n",
    "    # Step 5: Split the documents using the chosen splitter\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "-------------------------------------\n",
    "Text splitter\n",
    "chunk_size determines the maximum number of characters in each chunk after splitting the text.\n",
    "chunk_overlap is the number of characters that overlap between consecutive chunks. This ensures important information that might fall near chunk boundaries is not lost when splitting.\n",
    "-------------------------------------\n",
    "\"\"\"\n",
    "def text_splitter(document, chunk_size,chunk_overlap, option):\n",
    "    # Choose a text splitter based on the option provided\n",
    "    if option == 0:\n",
    "        # Recursive Character Splitter\n",
    "        \"\"\"\n",
    "        A recursive character text splitter is more intelligent than a simple character-based splitter. It tries to split the text at higher-level semantic boundaries first (e.g., paragraphs, sentences) and falls back to character splitting only if necessary.\n",
    "\n",
    "        Pros:\n",
    "        Maintains Context: It attempts to split on more logical breaks (e.g., between paragraphs) to preserve semantic meaning.\n",
    "        Flexibility: Falls back to finer-grained splits when higher-level splits are too long.\n",
    "        Overlapping Chunks: It allows for overlapping text between chunks, which can be useful for preserving context in downstream tasks like QA or retrieval.\n",
    "        \n",
    "        Cons:\n",
    "        Complexity: More computationally expensive and slower compared to simple character-based splitters.\n",
    "        Less Predictable Chunk Sizes: Depending on the structure of the document, chunk sizes may vary.\n",
    "        \n",
    "        Use Case:\n",
    "        Ideal for documents where maintaining semantic boundaries is important, such as legal documents or scientific papers.\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    elif option == 1:\n",
    "        # Simple Character Splitter\n",
    "        \"\"\"\n",
    "            A character-based text splitter splits text based on character count. This method cuts the text at a certain character length while avoiding breaking words in the middle.\n",
    "\n",
    "            Pros:\n",
    "            Simplicity: Easy to implement and ensures that you don't exceed token limits.\n",
    "            Control: You can precisely control the size of the text chunks by adjusting the character limit.\n",
    "            Consistent Chunk Size: It guarantees that chunks are consistently sized.\n",
    "\n",
    "            Cons:\n",
    "            Lacks Semantic Understanding: It may cut the text at inappropriate points, such as between sentences, paragraphs, or logical sections.\n",
    "            May Break Context: Since it doesn’t account for semantic boundaries, important context may get split across chunks.\n",
    "            \n",
    "            Use Case:\n",
    "            Simple text splitting for applications like summarization where chunk boundaries are less important.\n",
    "        \"\"\"\n",
    "        splitter = CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=chunk_size,chunk_overlap=chunk_overlap,length_function=len,is_separator_regex=False)\n",
    "    elif option == 2:\n",
    "        # Semantic Chunking based on Sentence Transformers\n",
    "        \"\"\"\n",
    "        Semantic chunking groups text into chunks based on the semantic meaning or similarity between sentences, instead of purely relying on token or character count. It uses models like Sentence-BERT to compute sentence embeddings and organizes sentences with similar meanings into the same chunk.\n",
    "\n",
    "        Pros:\n",
    "        Maintains Contextual Integrity: By grouping semantically related sentences together, it helps preserve the meaning of the text and ensures that ideas and concepts remain intact.\n",
    "        Improved Performance for NLP Tasks: For tasks like question answering and summarization, it can improve the performance of models by providing them with semantically coherent chunks.\n",
    "        Handles Complex Documents: Works well for complex documents where it's important to keep related content together, such as research papers, legal documents, or technical reports.\n",
    "        \n",
    "        Cons:\n",
    "        Computationally Expensive: Requires calculating sentence embeddings, which can be slow and resource-intensive, especially for large documents.\n",
    "        Inconsistent Chunk Sizes: Chunk sizes may vary significantly since it's based on semantic similarity rather than strict token or character limits.\n",
    "        Less Predictable: The chunks might not be evenly distributed in terms of size or structure, which can complicate downstream tasks if consistency is needed.\n",
    "        \n",
    "        Use Case:\n",
    "        Semantic chunking is ideal for use cases where maintaining the relationship between sentences is crucial, such as question answering, semantic search, and summarization of complex, structured documents (e.g., technical manuals, legal texts, or academic papers).\n",
    "                \n",
    "        \"\"\"\n",
    "        splitter = SemanticChunker(AzureOpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "                                                         azure_endpoint=\"https://langchai-introduction.openai.azure.com/\",\n",
    "                                                         api_key=\"API KEY HERE\",\n",
    "                                                         openai_api_version=\"2023-03-15-preview\"))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option. Choose between 0 (Recursive), 1 (Character),  2 (Semantic Chunking).\")\n",
    "    \n",
    "    return splitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "--------------------------------------\n",
    "Embedding / Vector Store Generation\n",
    "--------------------------------------\n",
    "\"\"\"\n",
    "def embed_document(doc_chunks):\n",
    "    \"\"\"\n",
    "    The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself). \n",
    "    .embed_query will return a list of floats, whereas .embed_documents returns a list of lists of floats.\n",
    "    \n",
    "    \"\"\"\n",
    "    azure_openai_embedding = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "                                                    azure_endpoint=\"https://langchai-introduction.openai.azure.com/\",\n",
    "                                                    api_key=\"API KEY HERE\",\n",
    "                                                    openai_api_version=\"2023-03-15-preview\")\n",
    "    doc_embeddings = azure_openai_embedding.embed_documents([doc.page_content for doc in doc_chunks]) #embed document\n",
    "    #embeddings = [azure_openai_embedding.embed_query(doc.page_content) for doc in doc_chunks] #embed query\n",
    "    vector_store = FAISS.from_texts([doc.page_content for doc in doc_chunks], azure_openai_embedding)\n",
    "    vector_store.save_local('faiss_index')\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Got a question for me? Please mention it in the comments section and I will get back to you.\n",
      "\n",
      "Upcoming Batches For DevOps Certification Training Course\n",
      "\n",
      "Docker has gained immense popularity in this fast growing IT world. Organizations are continuously adopting it in their production environment. I take this opportunity to explain Docker in the most simple way. In this blog, the following concepts will be covered:\n",
      "\n",
      "History Before containerization\n",
      "Reasons to use containers\n",
      "What is Docker?\n",
      "Dockerfile, Images  & Containers\n",
      "Docker Compose & Docker Swarm\n",
      "Hands-On\n",
      "Since Docker is a containerization platform before I tell you about it, it’s important that you understand the history behind containerization.\n",
      "\n",
      "History Before Containerization\n",
      "Before containerization came into the picture, the leading way to isolate, organize applications and their dependencies was to place each and every application in its own virtual machine. These machines run multiple applications on the same physical hardware, and this process is nothing but Virtualization.\n",
      "\n",
      "1. Introduction to Docker\n",
      "Docker is an open-source platform that automates the deployment, scaling, and management of applications inside lightweight containers. It allows developers to package an application with all of its dependencies, ensuring consistency across different environments. \n",
      "The concept of containers itself isn’t new— it originates from operating-system-level virtualization that has been around in forms such as jails, zones, and chroot. However, Docker made containers more accessible, lightweight, and portable for developers.\n",
      "The history of Docker begins with its launch by Solomon Hykes in 2013 under the Docker, Inc. umbrella. Initially, Docker utilized LXC (Linux Containers) but soon transitioned to its own library, libcontainer. Since then, Docker has evolved into a core tool in DevOps and cloud-native development, revolutionizing the way software is built and shipped.\n",
      "\n",
      "Docker Image: In layman terms, Docker Image can be compared to a template which is used to create Docker Containers. So, these read-only templates are the building blocks of a Container. You can use docker run to run the image and create a container.\n",
      "\n",
      "Docker Images are stored in the Docker Registry. It can be either a user’s local repository or a public repository like a Docker Hub which allows multiple users to collaborate in building an application.\n",
      "\n",
      "Docker Container: It is a running instance of a Docker Image as they hold the entire package needed to run the application. So, these are basically the ready applications created from Docker Images which is the ultimate utility of Docker.\n",
      "\n",
      "Now, that you know the basics, if you want to learn about the architecture of this technology then you can click here.\n",
      "Human: Tell me about the document?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Tell me about the document?\n",
      "Assistant: I'm sorry, but I don't have enough information to answer your question. Could you please provide me with more context or specify which document you are referring to?\n",
      "Follow Up Input: Why use Docker?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Docker's importance lies in the fact that it enables developers to ship code faster and more reliably by creating isolated environments that mimic production environments closely. This mitigates the common \"works on my machine\" problem in development.\n",
      "\n",
      "1. Introduction to Docker\n",
      "Docker is an open-source platform that automates the deployment, scaling, and management of applications inside lightweight containers. It allows developers to package an application with all of its dependencies, ensuring consistency across different environments. \n",
      "The concept of containers itself isn’t new— it originates from operating-system-level virtualization that has been around in forms such as jails, zones, and chroot. However, Docker made containers more accessible, lightweight, and portable for developers.\n",
      "The history of Docker begins with its launch by Solomon Hykes in 2013 under the Docker, Inc. umbrella. Initially, Docker utilized LXC (Linux Containers) but soon transitioned to its own library, libcontainer. Since then, Docker has evolved into a core tool in DevOps and cloud-native development, revolutionizing the way software is built and shipped.\n",
      "\n",
      "Docker: An In-Depth Overview\n",
      "\n",
      "7. Real-World Use Cases\n",
      "Docker has a variety of use cases:\n",
      "- **CI/CD Pipelines**: Docker streamlines continuous integration/continuous deployment (CI/CD) pipelines by providing consistent environments, ensuring that builds are repeatable across development, testing, and production.\n",
      "- **Microservices Architecture**: Docker is perfect for microservices because it allows services to run independently in containers. Each microservice can be scaled, updated, and deployed separately without affecting others.\n",
      "- **Platform Independence**: Docker containers can run on any system that supports Docker, making applications highly portable across development, staging, and production environments.\n",
      "- **Development Environments**: Developers use Docker to set up local development environments quickly, replicating production conditions on their machines without the need for complex setup.\n",
      "Human: What is the purpose of using Docker?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: Tell me about the document?\n",
      "Assistant: I'm sorry, but I don't have enough information to answer your question. Could you please provide me with more context or specify which document you are referring to?\n",
      "Human: Why use Docker?\n",
      "Assistant: The purpose of using Docker is to enable developers to package an application with all of its dependencies, ensuring consistency across different environments, and to create isolated environments that mimic production environments closely. This allows developers to ship code faster and more reliably, mitigating the common \"works on my machine\" problem in development. Docker also streamlines continuous integration/continuous deployment (CI/CD) pipelines, is perfect for microservices architecture, and allows for highly portable applications across development, staging, and production environments. Additionally, developers can use Docker to set up local development environments quickly, replicating production conditions on their machines without the need for complex setup.\n",
      "Follow Up Input: What was my first question?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Got a question for me? Please mention it in the comments section and I will get back to you.\n",
      "\n",
      "Upcoming Batches For DevOps Certification Training Course\n",
      "\n",
      "Docker Image: In layman terms, Docker Image can be compared to a template which is used to create Docker Containers. So, these read-only templates are the building blocks of a Container. You can use docker run to run the image and create a container.\n",
      "\n",
      "Docker Images are stored in the Docker Registry. It can be either a user’s local repository or a public repository like a Docker Hub which allows multiple users to collaborate in building an application.\n",
      "\n",
      "Docker Container: It is a running instance of a Docker Image as they hold the entire package needed to run the application. So, these are basically the ready applications created from Docker Images which is the ultimate utility of Docker.\n",
      "\n",
      "Now, that you know the basics, if you want to learn about the architecture of this technology then you can click here.\n",
      "\n",
      "2. Understanding Docker Architecture\n",
      "Docker operates on a client-server architecture.\n",
      "- **Docker Engine**: Docker Engine is the core part of Docker. It creates and manages Docker containers. The engine has two parts: the Docker Daemon, which handles the management and execution of containers, and the Docker CLI, which allows users to interact with Docker via commands.\n",
      "- **Docker Images**: An image is essentially a blueprint of a container. It includes everything needed to run an application – the code, the runtime, libraries, environment variables, and configuration files. Images are created using Dockerfiles, which define the steps to build an image.\n",
      "- **Docker Containers**: A container is an instance of a Docker image. Containers are isolated but lightweight, using shared operating system resources. They encapsulate the application and its dependencies, ensuring consistency across various systems.\n",
      "\n",
      "Docker has gained immense popularity in this fast growing IT world. Organizations are continuously adopting it in their production environment. I take this opportunity to explain Docker in the most simple way. In this blog, the following concepts will be covered:\n",
      "\n",
      "History Before containerization\n",
      "Reasons to use containers\n",
      "What is Docker?\n",
      "Dockerfile, Images  & Containers\n",
      "Docker Compose & Docker Swarm\n",
      "Hands-On\n",
      "Since Docker is a containerization platform before I tell you about it, it’s important that you understand the history behind containerization.\n",
      "\n",
      "History Before Containerization\n",
      "Before containerization came into the picture, the leading way to isolate, organize applications and their dependencies was to place each and every application in its own virtual machine. These machines run multiple applications on the same physical hardware, and this process is nothing but Virtualization.\n",
      "Human: What did the human ask about the document?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import trace\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the path to the .env file\n",
    "env_path = Path(r'C:\\Users\\AnumKhattak\\OneDrive - Revolution Data Platforms\\Documents\\LangChain\\Examples\\.env')\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Create the AzureChatOpenAI model instance\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup the conversation chain\n",
    "def setup_conversation_chain(vector_store):\n",
    "    # Memory for the conversation\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    # Create the ConversationalRetrievalChain\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return conversation_chain\n",
    "\n",
    "# Function to ask the chatbot a question\n",
    "def ask_chatbot(question, conversation_chain):\n",
    "    response = conversation_chain.run({\"question\": question})  # Fix the input key to match the prompt template\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "azure_openai_embedding = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "                                                    azure_endpoint=\"https://langchai-introduction.openai.azure.com/\",\n",
    "                                                    api_key=\"API KEY HERE\",\n",
    "                                                    openai_api_version=\"2023-03-15-preview\")\n",
    "document_path = r'C:\\Users\\AnumKhattak\\OneDrive - Revolution Data Platforms\\Documents\\LangChain\\Examples\\document'\n",
    "\n",
    "# Call document transformation function (ensure this function is defined elsewhere)\n",
    "split_data = document_transformation_text(document_path)\n",
    "\n",
    "# Embed the document into a vector store (ensure embed_document is defined)\n",
    "vector_store = embed_document(split_data)\n",
    "\n",
    "# Set up the conversation chain\n",
    "conversation_chain = setup_conversation_chain(vector_store)\n",
    "input_data = \"Tell me about the document?\"\n",
    "\n",
    "# Ask the chatbot a question\n",
    "response = ask_chatbot(input_data , conversation_chain)\n",
    "\n",
    "input_data = \"Why use Docker?\"\n",
    "# Ask the chatbot a question\n",
    "response = ask_chatbot(input_data , conversation_chain)\n",
    "\n",
    "input_data = \"What was my first question?\"\n",
    "# Ask the chatbot a question\n",
    "response = ask_chatbot(input_data , conversation_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_openai_embedding = AzureOpenAIEmbeddings(model=\"text-embedding-3-small\",\n",
    "                                                    azure_endpoint=\"https://langchai-introduction.openai.azure.com/\",\n",
    "                                                    api_key=\"API KEY HERE\",\n",
    "                                                    openai_api_version=\"2023-03-15-preview\")\n",
    "path = r'C:\\Users\\AnumKhattak\\OneDrive - Revolution Data Platforms\\Documents\\LangChain\\Examples\\document'\n",
    "split_data = document_transformation_text(path)\n",
    "\n",
    "vector_store = embed_document(split_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker: An In-Depth Overview\n",
      "Docker's importance lies in the fact that it enables developers to ship code faster and more reliably by creating isolated environments that mimic production environments closely. This mitigates the common \"works on my machine\" problem in development.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the docker?\"\n",
    "query_embedding = azure_openai_embedding.embed_query(query)\n",
    "\n",
    "# Search for the most similar documents in the vector store\n",
    "similar_docs = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# Display the retrieved documents\n",
    "for doc in similar_docs:\n",
    "    print(doc.page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\"\"\"\n",
    "--------------------------------------\n",
    "Observability with LangSmith\n",
    "--------------------------------------\n",
    "\"\"\"\n",
    "# Set LangSmith observability environment variables\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"API KEY\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "\n",
    "openai_client = wrap_openai(\n",
    "    OpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  # Set this in your environment\n",
    "        api_base=\"API BASE\",  # Your Azure resource base\n",
    "        api_type=\"azure\",\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")  # Set API version in environment\n",
    "    )\n",
    ")\n",
    "\n",
    "openai_client.invoke(\"What is 2+3?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
